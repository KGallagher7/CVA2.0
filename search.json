[
  {
    "objectID": "content/exposure.html",
    "href": "content/exposure.html",
    "title": "Species Exposure",
    "section": "",
    "text": "Species exposure will be quantified using decadal and long term projections from the MOM6 model. In the CVA1.0, exposure to surface temperatures and changes in ocean acidification were quantified using a z-score. This quantity describes how many standard deviations over the present mean conditions a value, in this case mean future conditions, are.\nFor the CVA2.0, we will use this same metric. Z-scores will be calculated across space and time on monthly timesteps for the decadal and eventually long-term predictions. Exposure values will be averaged within each month to produce a single exposure metric for each variable, and across time and space, using predicted species distributions to perform weighted averages so that only expsoure within the species ranges are considered. To calculate a total exposure across environmental variables, we will use relative variable importance from the ensemble models to perform weighted averages of the time series and maps across important covariates in the model.\nWe will also quantify changes in habitat suitability over time by projecting the ensemble species distribution model onto future climate conditions. The projections and the models themselves will allow us to examine the drivers of change, by looking at change in important envrionmental covariates.",
    "crumbs": [
      "Methods",
      "Species Exposure"
    ]
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/sensitivity.html",
    "href": "content/sensitivity.html",
    "title": "Sensitivity",
    "section": "",
    "text": "Species sensitivity to change will be measured qualitativly as in the original CVA. Experts will rank the impacts of different life history attributes on species sensitivity. A literature review is currently underway to determine the important life history traits that were both important to the original CVA and are important for target species.\nWe are also currently exploring options to quantify sensitivity from the MOM6 output following some of the methods of Boyce et al 2022 and 2024 such as margins between predicted or current conditions and known maximum environmental limits.\n\nSensitivity vs Adaptability\nThe Boyce et al 2022 and 2024 papers seperated species sensitivity and adaptability attributes. In the original CVA, sensitivity and adaptive attributes were grouped together, as removing one or the other significantly changed the results. Furthermore, Morrison et al 2015 considered sensitivity and adaptivity inverses of each other (ie higher sensitivity would mean that a species is less adaptive) and decided to group them.",
    "crumbs": [
      "Methods",
      "Sensitivity"
    ]
  },
  {
    "objectID": "content/sdm-methods.html",
    "href": "content/sdm-methods.html",
    "title": "Species Distribution Modeling",
    "section": "",
    "text": "The first step in the CVA2.0 is the construction of species distribution models (SDMs). SDMs are statistical models that describe the relationships between species presence or abundance and environmental attributes or covariates. There are many different types of SDMs that use different data types and statistical methods. There is an abundance of scientific literature that both describe and compare different model types. We will provide a brief overview below and describe the methods selected for the CVA2.0.",
    "crumbs": [
      "Methods",
      "Species Distribution Modeling"
    ]
  },
  {
    "objectID": "content/sdm-methods.html#presenceabsence-vs-abundance-data",
    "href": "content/sdm-methods.html#presenceabsence-vs-abundance-data",
    "title": "Species Distribution Modeling",
    "section": "Presence/Absence vs Abundance Data",
    "text": "Presence/Absence vs Abundance Data\nSpecies distribution models can be built with primarily two different kinds of species data - presence/absence or abundance data. Presence/absence data describes when a species was present or absent, usually in binary with 0s representing absences and 1s representing presences. SDMs built with presence/absence data predict how likely it is that a species will be found there - often called habitat suitability - but does not consider how many individuals will be present.\nAbundance data on the other hand uses count or species density estimates. SDMS built with abundance data can predict the abundance or density of species in a given area or at a given time, but must deal with often irregular data.\n\nIn the CVA2.0…\nFor the CVA2.0, we use presence/absence data from a variety of sources. Using presence/absence data will allow us to combine multiple datasets across fisheries independent and dependent surveys.",
    "crumbs": [
      "Methods",
      "Species Distribution Modeling"
    ]
  },
  {
    "objectID": "content/sdm-methods.html#statistical-modeling",
    "href": "content/sdm-methods.html#statistical-modeling",
    "title": "Species Distribution Modeling",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\nThere are many statistical models that we could have chosen for our SDMs. Options included generalized linear models (GLMs), generalized additive models (GAMs), uni (single) or multi-variate state-space models, machine learning methods, decision tree methods, and more! Many models have been used to describe species distributions along the Northeast Shelf. Each model type has pros and cons, and different models use different assumptions, statistical methods, etc to define habitat preferences.\nTo account for some of these differences between models, model ensembles can be generated by combining different model outputs together. Ensembles are usually built by averaging the outputs of different component models. Weighted averages can also be used to weight the averages based on model performance.\n\nIn the CVA2.0…\nWe use an ensemble modeling approach for the CVA2.0, following the work of the Alaska Fisheries Science Center (AFSC)’s Groundfish program, recently implimented an ensemble modeling framework for their Essential Fish Habitat work. Here, we use the following models:\n\nGeneralized Additive Model (GAM)\nMAXimum ENTropy (MAXENT)\nRandom Forest with Spatial Interpolation (RFSI)\nBoosted Regression Trees (BRTs)\nSpatio-temporal Generalized Linear Mixed Models (GLMMs) with sdmTMB\n\nAll of these models accept presence/absence data; can be made spatially and temporally explicit, which was important for our analysis; and combine more “traditional” SDM methods such as GAM and MAXENT included in the AFSC’s ensemble with machine learning/decision tree-based models such as RFSI and BRTs. sdmTMB models are similar to other SDM methods common in fisheries work like VAST and tinyVAST. In addition, these or similar models have been used on the Northeast Shelf in previously published works.\nWe will generate our ensemble model by performing a weighted average on our components’ outputs. Weights will be calculated using the same methods as the AFSC. However, we will use Area under the Curve (AUC) rather than root mean squared error (RMSE) because AUC is a better accuracy metric for presence/absence models than RMSE.",
    "crumbs": [
      "Methods",
      "Species Distribution Modeling"
    ]
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File &gt; New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools &gt; Global Options &gt; R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/ensemble-sdms.html",
    "href": "content/ensemble-sdms.html",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "",
    "text": "The first step in the CVA2.0 workflow is to build the species distribution models. These will be the basis of the exposure, directionality, and possibly additional indicators from the CVA. These are what allow the calculations to be spatially explicit and account for species habitat use.\nWithin this workflow, there are three steps:\n\nData preparation\nBuilding & Predicting Species Distribution Models\nBuilding & Predicting the Ensemble\n\nThe functions outlined below are designed to be run either sequentially or in parallel with the R parallelization method of your choice (doParallel and furrr are recommended) and can be run in loops across a list of species.",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#overview",
    "href": "content/ensemble-sdms.html#overview",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "",
    "text": "The first step in the CVA2.0 workflow is to build the species distribution models. These will be the basis of the exposure, directionality, and possibly additional indicators from the CVA. These are what allow the calculations to be spatially explicit and account for species habitat use.\nWithin this workflow, there are three steps:\n\nData preparation\nBuilding & Predicting Species Distribution Models\nBuilding & Predicting the Ensemble\n\nThe functions outlined below are designed to be run either sequentially or in parallel with the R parallelization method of your choice (doParallel and furrr are recommended) and can be run in loops across a list of species.",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#data-preparation",
    "href": "content/ensemble-sdms.html#data-preparation",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nFisheries Data\nThe function standardize_data can be used to standardize provided csv files or pull NEFSC survey or observer data from NEFSC databases via ROracle. This can also be accomplished in R outside of the package, but standardize_data provides a single function to format data in a way that will be accepted by downstream functions. If you are pulling together data from a variety of sources, you will likely still need to manipulate data outside of this function to make sure that data are in a single csv file and that dates and times are in the correct format.\nTo pull NEFSC survey or observer data, a channel from the package dbutils must be provided:\n\nsurv &lt;- standardize_data(dataType = 'Surveys', channel = dbutils::connect_to_database(server=\"NEFSC_pw_oraprod\",uid=\"KGALLAGHER\"))\n\nTo standardize CSV data, a file path to the csv file, as well as column names for station IDs; position (longitude/latitude); date; the column corresponding to the presence/absence, or count of the species; and species name.\n\nneamap &lt;- standardize_data(dataType = 'CSV', csv = \"./Data/csvs/raw/NEAMAP_Tow_Catch_2025-08-14.csv\", csvCols = c('station', 'lon', 'lat', 'date', 'present_absent', 'SCI_NAME'))\n\nThis function results in a dataframe being transformed from this:\n\n\n  X      ID TRIP_NUMBER.x HAUL_NUMBER.x AREA_FISHED SET_BEGIN_DATE SET_END_DATE\n1 1 44473.1         44473             1        3674      4/11/2010    4/11/2010\n2 2 44473.2         44473             2        3674      4/11/2010    4/11/2010\n3 3 44473.3         44473             3        3674      4/11/2010    4/11/2010\n4 4 44473.5         44473             5        3674      4/11/2010    4/11/2010\n5 5 44473.5         44473             5        3674      4/11/2010    4/11/2010\n6 6 44473.5         44473             5        3674      4/11/2010    4/11/2010\n  HAUL_BEGIN_DATE HAUL_END_DATE SET_BEGIN_LAT_CONV SET_BEGIN_LONG_CONV\n1       4/11/2010     4/11/2010             36.068             -74.921\n2       4/11/2010     4/11/2010             36.050             -74.909\n3       4/11/2010     4/11/2010             36.042             -74.942\n4       4/11/2010     4/11/2010             36.002             -74.975\n5       4/11/2010     4/11/2010             36.002             -74.975\n6       4/11/2010     4/11/2010             36.002             -74.975\n  HAUL_BEGIN_LAT_CONV HAUL_BEGIN_LONG_CONV HAUL_END_LAT_CONV HAUL_END_LONG_CONV\n1              36.063               74.921            36.065             74.936\n2              36.049               74.909            36.050             74.913\n3              36.044               74.941            36.044             74.945\n4              36.007               74.975            36.004             74.975\n5              36.007               74.975            36.004             74.975\n6              36.007               74.975            36.004             74.975\n  SET_DURATION HAUL_DURATION_HOURS GEAR_SOAK_HOURS  startDate VESSEL_ID\n1         0.07                0.90            3.08 2010-04-11    926397\n2         0.07                0.32            0.80 2010-04-11    926397\n3         0.03                0.18            0.48 2010-04-11    926397\n4         0.05                0.27            0.50 2010-04-11    926397\n5         0.05                0.27            0.50 2010-04-11    926397\n6         0.05                0.27            0.50 2010-04-11    926397\n  TRIP_NUMBER.y HAUL_NUMBER.y         SPECIES_NAME NUM_FISH SPECIES_ON_LIST\n1         44473             1             BLUEFISH      363               1\n2         44473             2             BLUEFISH       64               1\n3         44473             3             BLUEFISH        7               1\n4         44473             5             BLUEFISH       14               1\n5         44473             5 SHARK DOGFISH SMOOTH        3               0\n6         44473             5  SHARK DOGFISH SPINY        6               0\n\n\nTo this:\n\n\n  X       time month year   towID     lon    lat       date count\n1 1 2010-04-11     4 2010 44473.1 -74.921 36.068 2010-04-11   363\n2 2 2010-04-11     4 2010 44473.2 -74.909 36.050 2010-04-11    64\n3 3 2010-04-11     4 2010 44473.3 -74.942 36.042 2010-04-11     7\n4 4 2010-04-11     4 2010 44473.5 -74.975 36.002 2010-04-11    14\n5 5 2010-04-11     4 2010 44473.5 -74.975 36.002 2010-04-11     3\n6 6 2010-04-11     4 2010 44473.5 -74.975 36.002 2010-04-11     6\n                  name\n1             BLUEFISH\n2             BLUEFISH\n3             BLUEFISH\n4             BLUEFISH\n5 SHARK DOGFISH SMOOTH\n6  SHARK DOGFISH SPINY\n\n\nTo ensure that fisheries and environmental data are on the same scale, and to convert any abundance data to presence/absence, the fisheries csvs produced by standardize_data are then converted into rasters of effort/presence/absence with the function create_rast. While the source datasets are generated for the entire dataset, one rasterStack is produced for each target species and data source. create_rast produces rasters with the same resolution as the environmental data where values range from 0 to 2, where 0 means that no effort was present in the grid cell, 1 means that there was fishing effort but the target species was not caught, and 2 indicates that the target species was caught in that grid cell.\nUsers must provide the standardized csv files generated by standardize_data, a link to a netcdf file that the desired grid and timesteps can be extracted from, the start date of the timeseries, and a vector of alternate names for the target species to account for any differences in species names across data sources. The user must also indicate if the data come from fisheries independent (surveys) or dependent sources (observer or logbook programs). If data come from fisheries dependent sources, then a threshold is applied before the raster is created, following McHenry et al 2019; rasters are only created if the species is present at least 30 times throughout the dataset.\n\nrast &lt;- create_rast(data = csv, isObs = FALSE, grid = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/regrid/r20250715/tob.nwa.full.hcast.monthly.regrid.r20250715.199301-202312.nc\", origin = '1993-01-01', targetVec = \"Atlantic cod ,ATLANTIC COD ,Gadus morhua,Cod Atlantic,GADUS MORHUA\") #example to make raster for Atlantic Cod\n\nThe resulting rasterStack will have a range from 0-2 with a number of layers equal to the length of the timeseries. Below is an example layer from the produced rasterStack for Atlantic Cod from the NEFSC survey data. \nThe wrapper function saveRast is provided for create_rast. This wrapper function provides logging functionality to keep track of progress, which is especially helpful if running the code in parallel, and also adds a skip functionality to skip making the raster if it already exists for that species and data source. It also saves the resulting rasterStack in the input_rasters directory within the species-specific folder in the working directory.\nHere is an example of using saveRast in parallel using the package furrr with future_pmap:\n\nplan(multisession, workers = 5)\nfuture_pmap(list(..1 = argsList$csvName, ..2 = argsList$spp, ..3 = altNames, ..4 = argsList$skip, ..5 = argsList$isObs), ~ saveRast(csvName = ..1, spp = ..2, sppNames = ..3, skip = ..4, isObs = ..5, grid = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/regrid/r20230520/tob.nwa.full.hcast.monthly.regrid.r20230520.199301-201912.nc\", origin = '1993-01-01'), .progress = T)\nplan(sequential)\n\nWhere argsList is a dataframe with a row for each desired call of saveRast, and a column associated with most arguments for saveRast and create_rast. This is generated using tidyr::expand_grid to create a dataframe of every combination of target species and data sources.\ncreate_rast creates a seperate rasterStack for each species, which is saved to the species-specific input_rasters folder bv the saveRast wrapper function. This allows the user to examine the raster for each data source independently to ensure that it meets expectations. Before these can be matched with the environmental data, the rasters must be combined into a single rasterStack, while maintaining the 0-2 range to document presences, absences, and lack of effort. This is done with the merge_rasts and associated combineSave wrapper function. The combineSave wrapper function, in addition to providing logging and skip functionality, also automatically creates a list of raster objects in the input_rasters folder for the merge_rasts function.\n\ncombinedRasts &lt;- merge_rasts(rasts)\n#where rasts is a list of rasterStacks to combine\n\n\n\nEnvironmental Data\nThe example below illustrates how to grab and normalize monthly data from the Modular Ocean Model (MOM6) from the Northeast Shelf. The pull_hind and pull_forecast functions are designed to work with any MOM6 output. Other environmental data can be used as well, and normalized using the same functions, as long as the raw data is in the expected format.\nThe first step in getting the environmental data is pulling and spatially subsetting the raw MOM6 data. The functions provided default to pulling the entire time series of the specified model release. Functionality to subset the data by time may be added at a later date, as the length of the timeseries impacts how long the functions take the spatially subset the data.\nThe function pull_hind is designed to pull data from the hindcast simulations:\n\n r &lt;- pull_hind(jsonURL = \"https://psl.noaa.gov/cefi_portal/data_index/cefi_data_indexing.Projects.CEFI.regional_mom6.cefi_portal.northwest_atlantic.full_domain.hindcast.json\", reqVars = 'Bottom Temperature', shortNames = 'bottomT', release = 'r20230520', gt = 'regrid', of = 'monthly', bounds = c(-78,-65, 35,45), static = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/raw/r20230520/ocean_static.nc\")\n\nThe function ‘pull_forecast’ pulls data from the specified forecast similations:\n\n r &lt;- pull_forecast(jsonURL = \"https://psl.noaa.gov/cefi_portal/data_index/cefi_data_indexing.Projects.CEFI.regional_mom6.cefi_portal.northwest_atlantic.full_domain.decadal_forecast.json\", reqVars = 'Sea Water Potential Temperature at Sea Floor', shortNames = 'bottomT', release = 'r20250925', init = 'i202501', ens = 1, gt = 'regrid', of = 'monthly', bounds = c(-78,-65, 35,45), static = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/raw/r20230520/ocean_static.nc\")\n\nNote that pull_forecast has the additional arguments ‘init’ and ‘ens’, which represent the initalization date and the ensemble member desired. As of October 2025, the seasonal and decadal forecasts from MOM6 include 10 ensembles with slightly different initial forcing parameters, and as a result can be averaged across ensembles if desired, whereas the long-term forecasts have ensembles that represent different projected CO2 and climate conditions.\nFor either function, the JSON file link can be found on the CEFI portal in the ‘Data Access’ tab. The arguments reqVars, release, gt, of, init, and ens help specify which variables to pull, at what output frequency, and what grid to pull (raw vs regrid; the latter is recommended). These need to match their corresponding columns in the JSON file exactly. See the documentation for these functions for more details.\nSince spatially subsetting the data can take some time (upwards of 30 minutes per variable), if you are pulling a lot of environmental variables, it is recommended to run this command in parallel. Generally, this is not a memory-intensive activity; it just takes some time depending on the length of the timeseries (AKA the number of layers in the resulting rasterBrick). Below is an example of running the pull_hind function in parallel with doParallel:\n\ncluster &lt;- makeCluster(10, type='PSOCK')\nregisterDoParallel(cluster)\nraw &lt;- foreach(x = 1:nrow(var.list), .packages = c(\"ncdf4\", 'raster', 'jsonlite')) %dopar% {\n  r &lt;- pull_hind(varURL = \"https://psl.noaa.gov/cefi_portal/data_index/cefi_data_indexing.Projects.CEFI.regional_mom6.cefi_portal.northwest_atlantic.full_domain.hindcast.json\", reqVars = var.list$Long.Name[x], shortNames = var.list$Short.Name[x], release = 'r20230520')\n\n  raw[[x]] &lt;- r\n}\nstopCluster(cluster)\nnames(raw) &lt;- var.list$Short.Name\n\nWhere var.list is a two column data frame with the full names of the desired variable (Long.Name) and an abbreviated variable name (Short.Name) which will eventually become the column name in the final dataframe.\nOnce the raw hindcast or forecast data is pulled, the workflow moving forward is the same: mean and standard deviations are calculated monthly to calculate z-scores for the complete timeseries. It is recommended to normalize the data in some way prior to building the models to help with interpretation since all the environmental data will likely have different units. We chose to normalize using a z-score to be consistent with exposure calculation methods.\n\n# r is a list of rasterStacks of raw bottom temperature, bottom salinity, and bottom oxygen data \na &lt;- avg_env(r) #calculate monthly average\ns &lt;- sd_env(r) #calculate monthly standard deviation\nn &lt;- norm_env(rawList = r, avgList = a, sdList = s, shortNames = c('bottomT', 'bottomS', 'bottomO2')) #calculate z-score for every time step\n\nYou could also perform each of these steps in parallel following the example above. However, it is not really necessary since these steps are faster than the initial data pull. That being said, it is important to note that the avg_env, sd_env, and norm_env functions expect lists of rasterStacks (which would be the outcome of running pull_hind or pull_forecast in parallel as illustrated above).\nUnlike other functions in this package, no wrapper function is provided. It is not shown here but the resulting lists should be stored in the ./Data/MOM6/ folder so that the data pull only needs to be performed once; it is also recommended to save the raw, mean, and standard deviations, even if they aren’t used in the models. This will allow you to 1) easily examine the raw data and 2) calculate z-scores relative to different time series if you wish to do so (for example, normalize forecast data to the present-day mean and standard deviations).\nIf you have static environmental data such as bathymetry or distance to shore, these should also be normalized to their respective means and standard deviations. You should create an R object that is a named list of your static variables and put it in the ./Data/MOM6/ folder. Names will correspond to the columns in the data frame if you choose to include these data in your final data frame. Cropping these rasters to the same extent as the MOM6 data is helpful for managing file size. We do not provide functions for this process as these data can come from a variety of sources.\n\n\nCreating Data Frames\nAt this point, you should have two sets of raster files: one containing the fisheries effort/presence/absence data and another containing your environmental data. If you want to include static environmental covariates such as bathymetry, you will have a third set of raster files. While rasters are easy to plot, most of our ensemble model members do not take rasters as inputs, so these need to be converted to data frames.\nThe function merge_spp_env is provided to merge species and environmental rasters together, as well as add static environmental variables if provided.\n\ndf &lt;- merge_spp_env(rastStack = combinedRasts, envData = norm, addStatic = TRUE, staticData = './Data/staticVariables_cropped_normZ.RData')\n#combinedRasts is the combined rasterStack for the species created by the function 'merge_rasts'\n#staticData is the path to the RData object containing a list of static environmental rasters \n\nThe resulting data frame will have the following structure:\n\n\n'data.frame':   100 obs. of  26 variables:\n $ x         : num  -66.8 -66.7 -66.7 -66.6 -66.5 ...\n $ y         : num  45 45 45 45 45 ...\n $ variable  : chr  \"X01.1993\" \"X01.1993\" \"X01.1993\" \"X01.1993\" ...\n $ value     : num  0 0 0 0 0 0 0 0 0 0 ...\n $ month     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ year      : num  1993 1993 1993 1993 1993 ...\n $ bottomT   : num  0.182 0.179 0.178 0.172 0.17 ...\n $ bottomO2  : num  -0.14 -0.141 -0.147 -0.145 -0.148 ...\n $ bottomS   : num  0.00701 0.00852 0.01051 0.01072 0.01158 ...\n $ bottomArg : num  -0.000851 -0.000806 -0.000774 -0.000747 -0.000733 ...\n $ surfaceT  : num  0.123 0.126 0.129 0.127 0.125 ...\n $ surfaceS  : num  -0.02366 -0.01261 -0.00579 -0.00424 -0.00458 ...\n $ surfacepH : num  -0.1078 -0.1075 -0.1067 -0.1008 -0.0945 ...\n $ MLD       : num  -0.0925 -0.1174 -0.1207 -0.1069 -0.1362 ...\n $ diazPP    : num  0.132 0.137 0.143 0.145 0.145 ...\n $ smallPP   : num  0.038 0.0368 0.0471 0.0567 0.0567 ...\n $ mediumPP  : num  -0.0751 -0.0712 -0.0546 -0.0295 -0.0275 ...\n $ largePP   : num  -0.026092 -0.024324 -0.01777 -0.007 -0.000244 ...\n $ smallZoo  : num  -0.0764 -0.0848 -0.1159 -0.116 -0.172 ...\n $ mediumZoo : num  -0.0322 -0.0385 -0.0549 -0.0562 -0.0859 ...\n $ largeZoo  : num  0.0372 0.0369 0.0403 0.0377 0.0361 ...\n $ intNPP    : num  -0.043317 -0.03148 -0.016401 0.000677 0.000198 ...\n $ POC       : num  0.00419 -0.00173 -0.00279 -0.001 -0.00336 ...\n $ bathy     : num  -79 -51.6 -62.6 -57.6 -69.2 ...\n $ rugosity  : num  0.512 1.224 0.597 0.132 0.236 ...\n $ dist2coast: num  4.665 0.926 3.021 9.073 10.255 ...\n\n\nThe ‘x’, ‘y’, ‘variable’, ‘value’, ‘month’, ‘year’ columns will always be present. ‘variable’ is equal to the name of the rasterStack layers, which should be in MM.YYYY format to pull month and year. ‘value’ is the value in the fisheries raster grid cell and should be equal to 0, 1, or 2.\nAdditional functions are provided to clean/simplify the data frame: match_guilds, clean_data, and remove_corr. Of these, only clean_data is required, remove_corr is recommended, and match_guilds is optional.\nThese could be run in any order, with the exception of if the user is utlizing the match_guilds function. This should be run prior to remove_corr if using.\nHere, we will run match_guilds first. This function uses the habitat and feeding guilds keys to subset the columns in the data frame to only those relevant to the target species. For example, you may have environmental covariates from the ocean surface and sea floor, like surface and bottom temperatures. Species more associated with the sea floor, such as groundfish or flatfish, are more likely to be impacted by bottom temperatures than surface temperatures they may rarely interact with.\n\ndfG &lt;- match_guilds(spp_env = df, spp = c(\"PARALICHTHYS DENTATUS\"), spp_col = 'SCI_NAME', spp_guild = 'spp_list.csv', feeding_key = 'feeding_guilds.csv', feeding_col = 'Feeding.Guild', habitat_key = 'habitat_guilds.csv',  habitat_col = 'Habitat.Guild', static_vars = c('x', 'y', 'month', 'year', 'bathy', 'rugosity', 'dist2coast'), pa_col = 'value')\n\nFor example, based on the habitat and feeding keys provided, the data frame for Summer flounder will be subset to the following variables:\n\n\n'data.frame':   1000 obs. of  15 variables:\n $ x         : num  -66.8 -66.7 -66.7 -66.6 -66.5 ...\n $ y         : num  45 45 45 45 45 ...\n $ value     : num  0 0 0 0 0 0 0 0 0 0 ...\n $ month     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ year      : num  1993 1993 1993 1993 1993 ...\n $ bottomT   : num  0.182 0.179 0.178 0.172 0.17 ...\n $ bottomO2  : num  -0.14 -0.141 -0.147 -0.145 -0.148 ...\n $ bottomS   : num  0.00701 0.00852 0.01051 0.01072 0.01158 ...\n $ bottomArg : num  -0.000851 -0.000806 -0.000774 -0.000747 -0.000733 ...\n $ smallZoo  : num  -0.0764 -0.0848 -0.1159 -0.116 -0.172 ...\n $ mediumZoo : num  -0.0322 -0.0385 -0.0549 -0.0562 -0.0859 ...\n $ largeZoo  : num  0.0372 0.0369 0.0403 0.0377 0.0361 ...\n $ bathy     : num  -79 -51.6 -62.6 -57.6 -69.2 ...\n $ rugosity  : num  0.512 1.224 0.597 0.132 0.236 ...\n $ dist2coast: num  4.665 0.926 3.021 9.073 10.255 ...\n\n\nNext we will run clean_data to turn the data into “true” binary data. The fisheries rasters, and resulting data frames, had values ranging from 0 to 2, representing no effort (0), effort but no catch (1), and catch (2). For presence/absence modeling, the only accepted values are 0 and 1, and these models do not need to know where there was an absence of effort (while is generally good for users to know to understand the scope of the fishing effort). The function clean_data will remove all existing 0 values, and remap the presence and absence data to 1s and 0s.\n\ndfB &lt;- clean_data(dfG, pa_col = 'value')\n\nThe histograms below illustrate how the data have been transformed.\n\n\n\n\n\n\n\n\n\nNote that these datasets are random subsets of the complete dataset so the number of presences/absences is not equal in each. These histograms are only to illustrate the shift in range from 0-2 to 0-1.\nFinally, we will run remove_corr to remove any correlated environmental covariates before building the model. Position and time (month & year) are retained.\n\ndfC &lt;- remove_corr(dfB, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year')\n\nIf covariates are removed due to correlation with other covariates, the corresponding column names will be printed in the console or the log file.\nThe wrapper function makeDF includes all of the functions related to making and cleaning the data frame and provides skip and log file functionalities. This function also makes sure that the combined fisheries raster is loaded and the layers have the appropriate names. Like other wrapper functions, this function also saves the results from each call in the appropriate species-specific folder.\nThis function is generally not very computationally expensive (takes &lt;10 minutes), unless you are matching a lot of environmental covariates. If you need to perform this operation for a long list of species, the wrapper function is designed to be run in parallel.\n\nload('./Data/MOM6/norm_MOM6_092025.RData') #normalized environmental data needs to be loaded prior to launching the code in parallel\noptions(future.globals.maxSize = Inf) #remove check for sharing large files so that norm is shared across workers since this is a relatively low memory intensive job otherwise\nplan(multisession, workers = 5)\ndfs &lt;- future_pmap(list(..1 = args$name, ..2 = args$skip, ..3 = args$mMin, ..4 = args$mMax, ..5 = args$yMin, ..6 = args$yMax), ~ makeDF(name = ..1, skip = ..2, mMin = ..3, mMax = ..4, yMin = ..5, yMax = ..6), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#building-predicting-species-distribution-models",
    "href": "content/ensemble-sdms.html#building-predicting-species-distribution-models",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "Building & Predicting Species Distribution Models",
    "text": "Building & Predicting Species Distribution Models\nNow that the data frames have been constructed and cleaned, you can build the species distribution models. A series of functions are provided to build, perform cross-validation on, evaluate, and extract variable importance from the models for each of the component models for the ensemble: Generalized Additive Model (GAM), MAXENT, Random Forest (RF), Boosted Regression Trees (BRT), sdmTMB.\nThe first step is building the base model:\n\n mod &lt;- make_sdm(se = dfC, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year', model = model)\n\nmodel is one of the five component models: gam, maxent, rf, brt, sdmtmb or the ensemble (ens). This function first creates the base model and simplifies it as necessary.\nThe sdm_cv function performs a cross-validation of the model with k = 5.\n\n cv &lt;- sdm_cv(mod = mod, se = dfC, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year', model = model)\n\nThe predicted values are extracted from the cross-validation output to help build the ensemble and calculate the evaluation metric using the sdm_preds function.\n\n preds &lt;- sdm_preds(cv = cv, model = model)\n\nThe model is then evaluated using the predicted values. This function can calculate a Root Mean Squared Error (RMSE) or Area under the Curve (AUC).\n\n ev &lt;- sdm_eval(preds = preds, metric = 'auc', model = model)\n\nVariable importance for each of the ensemble model components can be extracted from the models:\n\nimp &lt;- sdm_importance(mod = mod, se = dfC, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year', model = model)\n\nIt is important to note that variable importance from the different models should not be compared without normalizing the values between 0 and 1 as they are all generated using different methods.\nThe wrapper function makeMods combines the above 5 functions, and includes skip and logging functionality. Like previous wrapper functions, it is easy to launch this in parallel with the furrr functionality.\n\nplan(multisession, workers = 5)\nchecks &lt;- future_pmap(list(..1 = args$spp, ..2 = args$model, ..3 = args$skip), ~ makeMods(spp = ..1, model = ..2, skip = ..3), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)\n\nWhere args is the combination of target species and models to be run, along with the flag to enable the skip functionality.\nHere, it is important to note that the different models have different memory demands. The GAM, RF, and MAXENT models are the least computationally expensive, followed by BRT. The sdmTMB is the most expensive. Run times will depend on the dataset size, computational resources, and your R environment. Therefore, it is recommended that you generate sdmTMB models in sequence, but all other models can be generated in parallel.\nBelow are example run times and run conditions for the final model runs for the Northeast CVA2.0 with 36 target species. All models were run in a remote R terminal, in an environment linked to Intel’s oneAPI, which includes a BLAS library, (sdmTMB recommends linking R to an optimized BLAS library to improve runtimes; see the sdmTMB installation guide) with 96 GB of available memory. Final datasets for each species had 100,000+ observations and were not downsampled, with the exception of the random forest models which perform better with even distributions of presences and absences.\n\nModel Run Times\n\n\n\nGAM\nMAXENT*\nRF\nBRT\nsdmTMB\n\n\n\n\nNumber of Cores\n10\n\n6\n10\n\n\n\nTotal Run Time (hrs)\n15\n\n2\n\n\n\n\nAverage Model Run Time (hrs)*\n4.2\n\n0.3\n\n\n\n\n\n\nNote 1: These are average run times calculated by dividing the total run time by the number of models run per core (36/number of cores). Some individual models were built faster or slower than the times listed due to the number of presences and the complexity of the environmental relationships\n*Note 2: The addition of the oneAPI BLAS library increased MAXENT model run times in comparison to previous runs in environments not linked to oneAPI. These previous MAXENT run times were more similar to GAM run times. However, sdmTMB run times decreased by ~2x (based on small scale tests), but needed to be run in sequence due to memory constraints (yes, even with 96GB of memory, there were memory contrains due to the size of the data set and memory needs of the model), whereas MAXENT models were memory-light enough to be run in parallel.\n\nThe function make_predictions will predict the model to a provided timeseries. It has the option to mask off the predictions based on bathymetry.\n\nload('./Data/MOM6/norm_MOM6_082025.RData') #list of normalized environmental rasterStacks to predict to; number of layers will dictate the time series predicted to\nabund &lt;- make_predictions(mod = mod, model = model, rasts = norm, mask = T, bathy_nm = 'bathy', bathy_max = 1000, se = dfC, staticData = './Data/staticVariables_cropped_normZ.RData', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year')\n\nThe output will be a rasterStack with the same resolution and number of layers as the time series of environmental data provided.\nThe wrapper function predictMods loads the necessary data frames and models, and saves the output to the output_rasters directory within the working directory. Similar to other wrapper functions, it also includes skip and logging functionality, and it is written to be easily run in parallel.\n\nplan(multisession, workers = 5)\nchecks &lt;- future_pmap(list(..1 = args$spp, ..2 = args$model, ..3 = args$skip), ~ predictMods(spp = ..1, model = ..2, skip = ..3), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)\n\nIt is important to note that for whatever reason, the MAXENT model cannot be predicted in parallel, so it must be run in sequence. However, predicting the models is much quicker and less memory-intensive than making the models themselves.",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#building-predicting-ensemble",
    "href": "content/ensemble-sdms.html#building-predicting-ensemble",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "Building & Predicting Ensemble",
    "text": "Building & Predicting Ensemble\nThe ensemble is built using the framework from the EFHSDM package. Here it is included in the make_sdm function:\n\nens &lt;- make_sdm(model = 'ens', ensembleWeights = weights, ensemblePreds = pds)\n\nWhere the weights is a vector of model weights produced either by the EFHSDM function MakeEnsemble if using RMSE or another method and pds is a list of prediction outputs from the cross-validation of the models. The weights vector and predictions list must be in the same order to correctly assign weights to predictions.\nTo predict the ensemble model, the make_predictions function is used:\n\nabund &lt;- make_predictions(model = 'ens', rasts = abds, weights = weights, staticData = NULL, mask = F, bathy_nm = NULL, bathy_max = NULL, se = NULL, month_col = NULL, year_col = NULL, xy_col = NULL)\n\nWhere abds is a list of predicted rasters from each of the component models. Similar to building the model, the order of the rasters must match the order of the models in the weights vector so that the weights can be assigned appropriately. To predict the ensemble, which is just a weighted average of the component model predictions, only a list of predictor rasters and the model weights are required, so many of the other arguments are set to NULL.\nThe wrapper function makeEns is provided to combine both the creation and prediction of the ensemble. Creating and predicting the ensemble is fast and quick since it is just performing a weighted average of the existing data, so no skip functionality is provided, but logging functionality remains. This can be run in parallel if desired.\n\nplan(multisession, workers = 5)\nchecks &lt;- future_pmap(list(..1 = spp.list$Name), ~ makeEns(spp = ..1), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/sensitivity-calculation.html",
    "href": "content/sensitivity-calculation.html",
    "title": "Calculating Sensitivity",
    "section": "",
    "text": "Overview\nSensitivity is calculated from MOM6 forecast data, and utilizes predicted species distributions, and the relative importance of covariates in the ensemble models.\nDetailed methods coming soon.",
    "crumbs": [
      "R Package",
      "Calculating Sensitivity"
    ]
  },
  {
    "objectID": "content/data.html",
    "href": "content/data.html",
    "title": "Data",
    "section": "",
    "text": "We utilized two different data types for this work - fisheries independent and dependent data.\n\n\nFisheries independent data comes from sources outside of the fishing industry. These often come in the form of regular surveys run by federal or state-level management bodies. Traditionally, these surveys occur in the spring and fall. For the CVA2.0 in the Northeast, we used the following surveys:\n\nNOAA Northeast Ecosystems Surveys\nNortheast Area Monitoring and Assessment Program Nearshore Trawl\nMaine-New Hampshire Inshore Trawl Survey\nMassachusetts Division of Marine Fisheries Bottom Trawl Survey\nLong Island Sound Trawl Survey\nNew York Nearshore Trawl Survey\nNew Jersey Ocean Stock Assessment Survey\nDelaware Trawl Surveys\n\n\n\n\nFisheries dependent data is data collected by commercial and recreational fishers. In the United States, commercial fisheries data is collected by fisheries observers in the National Observer Program, Gillnet Observer Program, and Pelagic Observer Program. These are trained individuals who accompany fishing boats on their trips to collect data on the species caught. Data from recreational fisheries is collected using post-effort in person or phone surveys at different docks via the logbook and Large Pelagics Survey programs.\n\n\n\nFisheries independent and dependent data are collected using different methods - they could fish for different lengths of time, at different speeds, or use different equipment. This is especially true for fisheries dependent data, which may used specialized equipment depending on the species being targeted. Therefore, we cannot compare the number of individuals caught across these methods. As a result, we will only examine whether species are present or absent and will not model the number, or abundance, of species. While different fishing methods could impact the ability to catch a particular species, thus impacting our results, we believe that this approach will account for most differences and will assume that all datasets are sampling for long enough periods of time and appropriate equipment to sample all species.",
    "crumbs": [
      "Methods",
      "Data"
    ]
  },
  {
    "objectID": "content/data.html#fisheries-data",
    "href": "content/data.html#fisheries-data",
    "title": "Data",
    "section": "",
    "text": "We utilized two different data types for this work - fisheries independent and dependent data.\n\n\nFisheries independent data comes from sources outside of the fishing industry. These often come in the form of regular surveys run by federal or state-level management bodies. Traditionally, these surveys occur in the spring and fall. For the CVA2.0 in the Northeast, we used the following surveys:\n\nNOAA Northeast Ecosystems Surveys\nNortheast Area Monitoring and Assessment Program Nearshore Trawl\nMaine-New Hampshire Inshore Trawl Survey\nMassachusetts Division of Marine Fisheries Bottom Trawl Survey\nLong Island Sound Trawl Survey\nNew York Nearshore Trawl Survey\nNew Jersey Ocean Stock Assessment Survey\nDelaware Trawl Surveys\n\n\n\n\nFisheries dependent data is data collected by commercial and recreational fishers. In the United States, commercial fisheries data is collected by fisheries observers in the National Observer Program, Gillnet Observer Program, and Pelagic Observer Program. These are trained individuals who accompany fishing boats on their trips to collect data on the species caught. Data from recreational fisheries is collected using post-effort in person or phone surveys at different docks via the logbook and Large Pelagics Survey programs.\n\n\n\nFisheries independent and dependent data are collected using different methods - they could fish for different lengths of time, at different speeds, or use different equipment. This is especially true for fisheries dependent data, which may used specialized equipment depending on the species being targeted. Therefore, we cannot compare the number of individuals caught across these methods. As a result, we will only examine whether species are present or absent and will not model the number, or abundance, of species. While different fishing methods could impact the ability to catch a particular species, thus impacting our results, we believe that this approach will account for most differences and will assume that all datasets are sampling for long enough periods of time and appropriate equipment to sample all species.",
    "crumbs": [
      "Methods",
      "Data"
    ]
  },
  {
    "objectID": "content/data.html#environmental-data",
    "href": "content/data.html#environmental-data",
    "title": "Data",
    "section": "Environmental Data",
    "text": "Environmental Data\nWe will be using species distribution modeling to quantify the relationships between species presence and their environment. We are using the Modular Ocean Model (MOM) 6 hindcast data to drive these models. The MOM6 is developed by the Changing Fisheries and Ecosystem Initiative and includes both a physical ocean model and a coupled biogeochemical model.\n\nFor the CVA2.0\nWe are using the hindcast timeseries from 1993-2019 from the MOM6 for the Northwest Atlantic to build the species distribution models. We will then project these models into the future using the decadal and long-term projections from the MOM6.\nCovariates used to build the species distribution models include:\n\nSea Surface Temperature & Salinity\nBottom Temperature & Salinity\nSurface pH\nBottom Aragonite Solubility\nMixed Layer Depth\nDiazotroph, small, medium, and large phytoplankton primary productivity (integrated in the top 100 m)\nSmall, medium, and large zooplankton biomass (integrated in the top 100 m)\nDownward particulate organic carbon (POC) flux\nWater column net primary productivity (NPP)\n\nCovariates will be normalized to long term (1993 - 2019) averages and matched to species based on their habitat and prey preferences. For example, the species distribution model for summer flounder will be built using bottom temperature and salinity, bottom aragonite solubility, downward POC flux, and NPP.",
    "crumbs": [
      "Methods",
      "Data"
    ]
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#quarto-documentation",
    "href": "content/customizing.html#quarto-documentation",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "Examples",
    "text": "Examples\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFay lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "",
    "text": "The Climate Vulnerability Assessment (CVA) was originally performed by the Northeast Fisheries Science Center in 2015. This original work, which was later replicated by all the National Marine Fisheries Service (NMFS) regions, used expert opinions to rank how target species would be impacted by future climate change using both exposure and sensitivity attributes. For the original CVA in the northeast US, experts ranked how much change species would be exposed to using large glocal ocean model forecasts and how sensitive they would be to these changes. This resulted in a single exposure and sensitivity score for each species, which was combined to give an overall vulnerability score. Experts were also asked to rank the directionality of climate impacts. The original methods are described in Morrison et al 2015 and Hare et al 2016, and the original results for the northeast shelf can be found in Hare et al 2016. CVA results for all regions can be explored using the NOAA CVA Tool.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#history-of-the-climate-vulnerability-assessment",
    "href": "index.html#history-of-the-climate-vulnerability-assessment",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "",
    "text": "The Climate Vulnerability Assessment (CVA) was originally performed by the Northeast Fisheries Science Center in 2015. This original work, which was later replicated by all the National Marine Fisheries Service (NMFS) regions, used expert opinions to rank how target species would be impacted by future climate change using both exposure and sensitivity attributes. For the original CVA in the northeast US, experts ranked how much change species would be exposed to using large glocal ocean model forecasts and how sensitive they would be to these changes. This resulted in a single exposure and sensitivity score for each species, which was combined to give an overall vulnerability score. Experts were also asked to rank the directionality of climate impacts. The original methods are described in Morrison et al 2015 and Hare et al 2016, and the original results for the northeast shelf can be found in Hare et al 2016. CVA results for all regions can be explored using the NOAA CVA Tool.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#why-cva2.0",
    "href": "index.html#why-cva2.0",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "Why CVA2.0?",
    "text": "Why CVA2.0?\nThe original CVA produced single metrics for each species across their entire range. However, in the past decade, it has become clear that climate change is not uniform worldwide. Therefore, species or stocks may be more or less vulnerable in different portions of their range. As a result, there is a need for a spatially explicit CVA. In addition, the development of high resolution regional ocean models such as the MOM6 model also allow changes to be predicted on fine spatial and temporal scales. The availability of the MOM6 hindcast and forecast on both decadal and long-term scales, plus new information on life history attributes for species, will allow us to update the existing CVA.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "How to Use this Book",
    "text": "How to Use this Book\nThe purpose of this book is to document the methods and capture the decisions made in the development of the CVA2.0 for the Northeast Shelf, so that these methods could be broadly applied to other NMFS regions or management bodies. This book will also outline the R package that is actively in development to assist in reproducability across regions. Automatic reporting methods and any future RShiny application development to assist in gathering expert opinons will also be outlined here.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/install-setup.html",
    "href": "content/install-setup.html",
    "title": "Installation & Setup",
    "section": "",
    "text": "Installation instructions will come once the R package is finalized.",
    "crumbs": [
      "R Package",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "content/install-setup.html#installation",
    "href": "content/install-setup.html#installation",
    "title": "Installation & Setup",
    "section": "",
    "text": "Installation instructions will come once the R package is finalized.",
    "crumbs": [
      "R Package",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "content/install-setup.html#set-up",
    "href": "content/install-setup.html#set-up",
    "title": "Installation & Setup",
    "section": "Set Up",
    "text": "Set Up\n\nNecessary Files\n\nKey Files\nIf you wish to subset environmental covariates used in the models when making the data frames, you will need files that 1) list the species and their habitat and feeding guilds and 2) define the covariates to keep for each feeding and habitat guild, referred to as key files.\nThe species list should at least include a column with the species name and then columns for the associated feeding and habitat guild. While it is not necessary, this file is also a good place to list alternative species names (common names, scientific names, any variations on either) that can be used to make sure that observations of the same species are combined across data types.\n\nspp &lt;- read.csv('spp_list.csv')\nhead(spp)\n\n           Common.Name             COM_NAME           Scientific.Name\n1        Atlantic cod         ATLANTIC COD               Gadus morhua\n2     Atlantic croaker     ATLANTIC CROAKER   Micropogonias undulatus\n3     Atlantic halibut     ATLANTIC HALIBUT Hippoglossus hippoglossus\n4     Atlantic herring     ATLANTIC HERRING           Clupea harengus\n5    Atlantic mackerel    ATLANTIC MACKEREL          Scomber scombrus\n6 Atlantic sea scallop ATLANTIC SEA SCALLOP  Placopecten magellanicus\n     Alternate.Name                  SCI_NAME            SCI_NAME_ALT\n1      Cod Atlantic              GADUS MORHUA                        \n2  CROAKER ATLANTIC   MICROPOGONIAS UNDULATUS MICROPOGONIAS UNDULATES\n3  Halibut Atlantic HIPPOGLOSSUS HIPPOGLOSSUS        HALIBUT ATLANTIC\n4  Herring Atlantic           CLUPEA HARENGUS                        \n5 Mackerel Atlantic          SCOMBER SCOMBRUS       MACKEREL ATLANTIC\n6       Scallop Sea  PLACOPECTEN MAGELLANICUS PLACOPECTEN MAGELANICUS\n            SCI_NAME_ALT2 Managing.Body Feeding.Guild Habitat.Guild\n1                                 NEFMC     Piscivore    Groundfish\n2 MICROPOGONIUS UNDULATUS         ASMFC    Benthivore    Groundfish\n3                                 NEFMC     Piscivore    Groundfish\n4                                 NEFMC   Planktivore       Pelagic\n5                                 MAFMC   Planktivore      Pelagic \n6                                 NEFMC       Benthos       Benthic\n\n\nKey files should have seperate columns, for each feeding and habitat guild with names that match the different feeding and habitat guilds in the species list. The entries in each column should be a column name associated with that guild.\n\nfeed &lt;- read.csv('feeding_guilds.csv')\nhead(feed)\n\n  Planktivore Piscivore Benthos Benthivore Apex.Predator\n1      diazPP  smallZoo  intNPP     intNPP        intNPP\n2     smallPP mediumZoo     POC        POC              \n3    mediumPP  largeZoo                                 \n4     largePP                                           \n\n\n\nhab &lt;- read.csv('habitat_guilds.csv')\nhead(hab)\n\n  Groundfish   Benthic   Pelagic Pelagic.Migratory\n1    bottomT   bottomT  surfaceT          surfaceT\n2    bottomS   bottomS  surfaceS          surfaceS\n3   bottomO2  bottomO2 surfacepH         surfacepH\n4  bottomArg bottomArg       MLD               MLD\n\n\n\n\nFisheries Data\nThis workflow is designed to accept both fisheries independent and dependent data sources. Data standardization functions have the ability to pull NEFSC survey (using survdat) and observer data using ROracle, but can also accept CSV files. CSV files will need records of all efforts, regardless of if the target species is collected or not, to correctly document absences and have a time column that can be converted with the POSIXct function.\n\n\nEnvironmental Data\nFunctions are provided to pull hindcast or forecast data from MOM6 models via the CEFI portal. Other environmental data can be supplied, but should be on regular grids and are ideally netcdf files that can be read using raster if fisheries presence/absence rasters are desired on the same grid.\n\n\n\nDirectory\nThis R package is designed to work within a single working directory. To use the wrapper functions provided, the working directory should include a folder for each target species, a Data folder, and a logs folder. Key files can be added to the working directory.\nThe Data subdirectory should have subfolders for csv and MOM6 (or other environmental) data, as well as an object containing a list of static environmental covariates (bathymetry, distance to shore, etc). The CSV folder should contain raw and standardized subfolders to hold the raw csv files from different data sources, and their standardized counterparts from the standardize_data function. The MOM6 data should contain the raw, averaged, standard deviation, and normalized outputs from the MOM6 functions.\nEach species folder should contain three subfolders: 1) input_rasters, 2) model_output, and 3) output_rasters. The input_rasters folder contains all the individual rasters from each data source and the combined raster. The output_rasters folder contains all the predicted rasters from each model and the final ensemble model. The model_output folder contains the following folders: 1) models, 2) cvs, 3) preds, 4) eval_metrics, and 5) importance. Each of these folders will contain the output from their respective functions for each model component, and as necessary, the ensemble model.\n\n\n\nExample directory",
    "crumbs": [
      "R Package",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/add-content.html",
    "href": "content/add-content.html",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#edit-and-add-your-pages",
    "href": "content/add-content.html#edit-and-add-your-pages",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#add-your-pages-the-project",
    "href": "content/add-content.html#add-your-pages-the-project",
    "title": "Customize",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "content/vulnerability_directionality.html",
    "href": "content/vulnerability_directionality.html",
    "title": "Vulnerability & Directionality",
    "section": "",
    "text": "Vulnerability\nThe final vulnerability scores will be estimated using similar methods to Boyce et al 2022 and 2024 - a weighted average of sensitivity, exposure, and if used in this analysis, adaptive, scores based on variability across the study area. For attributes based on life history stages, such as sensitivity and/or adaptability, these will be uniform across the study region. These averages will occur across space.\nThe final product will be a map of species vulnerability across the study region. An overall vulnerability score may be generated based on habitat suitability.\n\n\nDirectionality\nIn the original CVA, in addition to ranking how sensitive species may be to future climate change, experts also estimated if species would be positively or negatively impacted by climate change. For the CVA2.0, we will use the species distribution models to estimate current area of preferred habitat. We will calculate change in habitat area over time from the decadal and long-term projections from MOM6. We may also calculate directionality via expert opinion and compare the two metrics.",
    "crumbs": [
      "Methods",
      "Vulnerability & Directionality"
    ]
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "The CVA2.0 project is funded by the Inflation Reduction Act.\nThis project would not have been possible without the dedication and hardwork of countless individuals aboard the federal and state-run survey programs and the NOAA Observer program.\nMany thanks to the following state-run programs for contributing data:\n\nNortheast Area Monitoring and Assessment Program Nearshore Trawl\nMaine-New Hampshire Inshore Trawl Survey\nMassachusetts Division of Marine Fisheries Bottom Trawl Survey\nLong Island Sound Trawl Survey\nNew York Nearshore Trawl Survey\nNew Jersey Ocean Stock Assessment Survey\nDelaware Trawl Surveys\n\nWe also thank all of the NOAA staff, members of the New England and Mid Atlantic Fisheries Management Councils, the Atlantic States Marine Fisheries Commission, and our academic partners for their feedback and assistance in developing the methods described here.\nThis website is based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "content/rpackage-overview.html",
    "href": "content/rpackage-overview.html",
    "title": "RPackage Overview",
    "section": "",
    "text": "An R package is undergoing active development to document the workflows and codes used to contruct the new Climate Vulnerability Assessment.\nThe package will have 4 major components:\n\nBuilding the ensemble species distribution models\nEstimating exposure\nSummarizing sensitivity from expert scores\nCalculating final vulnerability scores\n\nThe following pages provide a tutorial for the different portions of the package.",
    "crumbs": [
      "R Package"
    ]
  },
  {
    "objectID": "content/methods-overview.html",
    "href": "content/methods-overview.html",
    "title": "Methods Overview",
    "section": "",
    "text": "The CVA2.0 will take a semi-quantiative approach, using both quantitative and qualitative methods.\n\n\nSpecies distributions will be quantified using ensemble species distribution models, forced by environmental covariates from the MOM6 hindcast (1993 - 2019) and driven by fisheries dependent and independent datasets. These species distribution models will then be projected into the future using the MOM6 decadal and long-term forecasts to quantify changes in species distribution.\nWe will be able to use these same decadal and long-term (to 2100) forecasts to quantify changes in important environmental covariates over time, to determine drivers of change.\n\n\n\nSpecies sensitivity will be estimated following similar methods to the original CVA. We will perform a literature review to update our understanding of important life history attributes and solict expert feedback on the most important life history attributes.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "content/methods-overview.html#overview",
    "href": "content/methods-overview.html#overview",
    "title": "Methods Overview",
    "section": "",
    "text": "The CVA2.0 will take a semi-quantiative approach, using both quantitative and qualitative methods.\n\n\nSpecies distributions will be quantified using ensemble species distribution models, forced by environmental covariates from the MOM6 hindcast (1993 - 2019) and driven by fisheries dependent and independent datasets. These species distribution models will then be projected into the future using the MOM6 decadal and long-term forecasts to quantify changes in species distribution.\nWe will be able to use these same decadal and long-term (to 2100) forecasts to quantify changes in important environmental covariates over time, to determine drivers of change.\n\n\n\nSpecies sensitivity will be estimated following similar methods to the original CVA. We will perform a literature review to update our understanding of important life history attributes and solict expert feedback on the most important life history attributes.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  }
]