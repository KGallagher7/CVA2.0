[
  {
    "objectID": "content/exposure.html",
    "href": "content/exposure.html",
    "title": "Species Exposure",
    "section": "",
    "text": "Species exposure is quantified using the decadal from the MOM6 model. In the CVA1.0, exposure to surface temperatures, salinities, and changes in ocean acidification were quantified using a z-score. This quantity describes how many standard deviations over the present mean conditions a value, in this case mean future conditions, are.\nFor the CVA2.0, we will use this same metric. Z-scores will be calculated across space and time using average monthly timesteps. Exposure values will be averaged across time and space each month to produce an exposure map and monthly timeseries for each variable, using predicted species distributions to perform weighted averages so that only expsoure within the species ranges are considered and habitat use by the species is taken into account.\nWe will calculate total exposure using the same logic rule implimented in the CVA 1.0 in two different ways: 1) counting all ecologicaly relevant variables and 2) only counting variables that have relative weights over 10% in the species distribution models. This will be done across both time and space to produce both a total timeseries and map of exposure for each species.",
    "crumbs": [
      "Methods",
      "Species Exposure"
    ]
  },
  {
    "objectID": "content/sensitivity_calculation.html",
    "href": "content/sensitivity_calculation.html",
    "title": "Sensitivity Calculation",
    "section": "",
    "text": "Sensitivity scores were collected using the Fish Stock Climate Vulnerability Assessment Portal, and exported as a csv. Scores were calculated for each attribute and these scores were used to calculate a final exposure score for the species. Unlike exposure, and similar to the original analysis, a single value is produced for each species.\n\n\nAttribute scores were calculated using a weighted average following Equation 1 in Morrison et al 2015. Briefly, the number of tallies in each of the four bins is multiplied by a weight, summed, and divided by the total number of tallies (which would be 5 scorers per species x 5 tallies per scorer = 25). The weights that correspond to each bin are:\n\nBin Weights\n\n\nBin\nWeight\n\n\n\n\nLow\n1\n\n\nMedium\n2\n\n\nHigh\n3\n\n\nVery High\n4\n\n\n\nThe function attribute.score performs the weighted average. This function is based on code provided by the Tyler Loughran and the Atlantic Highly Migratory Species Management Division, Office of Sustainable Fisheries. The workflow is outlined below.\n\n\n\nFollowing the original CVA methods, total sensitivity is calculated with a logic rule. This is the same logic rule used to calculate total exposure from individual variable exposure scores:\n\nLogic Rule used to Calculate Total Species Sensitivity\n\n\nOverall Score\nNumeric Score\nLogic Rule\n\n\n\n\nLow\n1\nAll other scores\n\n\nMedium\n2\n2 or more attributes &gt;= 2.5\n\n\nHigh\n3\n2 or more attributes &gt;= 3.0\n\n\nVery High\n4\n3 or more attributes &gt;= 3.5\n\n\n\nThe total sensitivity score is calculated using the logic.rule function.\n\n\n\n\n\nA single function, calculate.sensitivity, combines both the attribute score calculation done by attribute.score and the total sensitivity score calculation done by logic.rule in a single function. This function is modeled on provided code from the Southeast Fisheries Science Center and built to use lapply functionality to move through all species quickly.\n\ndata &lt;-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal\nspecies.data.list &lt;- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list\nspecies.sensitivity &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = F) #calculate attribute scores and total sensitivity \n\nThe resulting data.frame contains a row for each species scored, with columns containing the attribute scores and a total sensitivity score.\n\n\n\nUncertainty was quantified using the tally-based scoring system described above. Uncertainty was calculated by resampling the tallies with replacement across the four bins, and recalculating the weighted average attribute score, then the total sensitivity. Following the original CVA, uncertainty is represented as the proportion or percent of bootstrapped total sensitivity scores that match the expert-derived total scores. For the default number of iterations (10,000), this function takes just under a minute (~45 seconds) per species on a standard workstation.\nBoth ‘attribute.score’ and ‘logic.rule’, and subsequently ‘calculate.sensitivity’ have the ability to also perform the bootstrap calculation.\n\ndata &lt;-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal\nspecies.data.list &lt;- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list\nsensitivity.bootstrap &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = T, samples = 10000) #calculate sensitivity with bootstrapping \n\nWhen bootstrap = TRUE, the resulting data.frame will have a number of rows equal to the number of samples, with each row representing one of the iterations. Columns will contain the attribute scores and total sensitivity score for that iteration.\nThe function bootstrap.certainty calculates the final uncertainty value (AKA the proportion of bootstrap iterations that match the expert scores). This function will calculate the proportion of bootstrapped total sensitivity scores correspond to each possible score (1-4). If match = TRUE, then the corresponding proportion to the score derived from the expert tallies will be appended to the output from calculate.sensitivity(bootstrap = TRUE) as a column called Certainty. If match = FALSE, a vector containing the proportion of bootstrap scores that correspond to each possible total sensitivity score is returned.\nAs such, the total workflow can be run with only 5 lines of code:\n\ndata &lt;-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal\nspecies.data.list &lt;- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list\nspecies.sensitivity &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = F) #calculate attribute scores and total sensitivity\nsensitivity.bootstrap &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = T, samples = 10000) #calculate sensitivity with bootstrapping \nsensitivity.certainty &lt;- lapply(sensitivity.bootstrap, bootstrap.certainty, match = T, sensitivity.dataframe = species.sensitivity)",
    "crumbs": [
      "R Package",
      "Sensitivity Calculation"
    ]
  },
  {
    "objectID": "content/sensitivity_calculation.html#analysis",
    "href": "content/sensitivity_calculation.html#analysis",
    "title": "Sensitivity Calculation",
    "section": "",
    "text": "Sensitivity scores were collected using the Fish Stock Climate Vulnerability Assessment Portal, and exported as a csv. Scores were calculated for each attribute and these scores were used to calculate a final exposure score for the species. Unlike exposure, and similar to the original analysis, a single value is produced for each species.\n\n\nAttribute scores were calculated using a weighted average following Equation 1 in Morrison et al 2015. Briefly, the number of tallies in each of the four bins is multiplied by a weight, summed, and divided by the total number of tallies (which would be 5 scorers per species x 5 tallies per scorer = 25). The weights that correspond to each bin are:\n\nBin Weights\n\n\nBin\nWeight\n\n\n\n\nLow\n1\n\n\nMedium\n2\n\n\nHigh\n3\n\n\nVery High\n4\n\n\n\nThe function attribute.score performs the weighted average. This function is based on code provided by the Tyler Loughran and the Atlantic Highly Migratory Species Management Division, Office of Sustainable Fisheries. The workflow is outlined below.\n\n\n\nFollowing the original CVA methods, total sensitivity is calculated with a logic rule. This is the same logic rule used to calculate total exposure from individual variable exposure scores:\n\nLogic Rule used to Calculate Total Species Sensitivity\n\n\nOverall Score\nNumeric Score\nLogic Rule\n\n\n\n\nLow\n1\nAll other scores\n\n\nMedium\n2\n2 or more attributes &gt;= 2.5\n\n\nHigh\n3\n2 or more attributes &gt;= 3.0\n\n\nVery High\n4\n3 or more attributes &gt;= 3.5\n\n\n\nThe total sensitivity score is calculated using the logic.rule function.\n\n\n\n\n\nA single function, calculate.sensitivity, combines both the attribute score calculation done by attribute.score and the total sensitivity score calculation done by logic.rule in a single function. This function is modeled on provided code from the Southeast Fisheries Science Center and built to use lapply functionality to move through all species quickly.\n\ndata &lt;-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal\nspecies.data.list &lt;- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list\nspecies.sensitivity &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = F) #calculate attribute scores and total sensitivity \n\nThe resulting data.frame contains a row for each species scored, with columns containing the attribute scores and a total sensitivity score.\n\n\n\nUncertainty was quantified using the tally-based scoring system described above. Uncertainty was calculated by resampling the tallies with replacement across the four bins, and recalculating the weighted average attribute score, then the total sensitivity. Following the original CVA, uncertainty is represented as the proportion or percent of bootstrapped total sensitivity scores that match the expert-derived total scores. For the default number of iterations (10,000), this function takes just under a minute (~45 seconds) per species on a standard workstation.\nBoth ‘attribute.score’ and ‘logic.rule’, and subsequently ‘calculate.sensitivity’ have the ability to also perform the bootstrap calculation.\n\ndata &lt;-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal\nspecies.data.list &lt;- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list\nsensitivity.bootstrap &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = T, samples = 10000) #calculate sensitivity with bootstrapping \n\nWhen bootstrap = TRUE, the resulting data.frame will have a number of rows equal to the number of samples, with each row representing one of the iterations. Columns will contain the attribute scores and total sensitivity score for that iteration.\nThe function bootstrap.certainty calculates the final uncertainty value (AKA the proportion of bootstrap iterations that match the expert scores). This function will calculate the proportion of bootstrapped total sensitivity scores correspond to each possible score (1-4). If match = TRUE, then the corresponding proportion to the score derived from the expert tallies will be appended to the output from calculate.sensitivity(bootstrap = TRUE) as a column called Certainty. If match = FALSE, a vector containing the proportion of bootstrap scores that correspond to each possible total sensitivity score is returned.\nAs such, the total workflow can be run with only 5 lines of code:\n\ndata &lt;-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal\nspecies.data.list &lt;- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list\nspecies.sensitivity &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = F) #calculate attribute scores and total sensitivity\nsensitivity.bootstrap &lt;- lapply(species.data.list, calculate.sensitivity, bootstrap = T, samples = 10000) #calculate sensitivity with bootstrapping \nsensitivity.certainty &lt;- lapply(sensitivity.bootstrap, bootstrap.certainty, match = T, sensitivity.dataframe = species.sensitivity)",
    "crumbs": [
      "R Package",
      "Sensitivity Calculation"
    ]
  },
  {
    "objectID": "content/methods-overview.html",
    "href": "content/methods-overview.html",
    "title": "Methods Overview",
    "section": "",
    "text": "The CVA2.0 will take a semi-quantiative approach, using both quantitative and qualitative methods.\n\n\nSpecies distributions will be quantified using ensemble species distribution models, forced by environmental covariates from the MOM6 hindcast (1993 - 2019) and driven by fisheries dependent and independent datasets. These species distribution models will then be projected into the future using the MOM6 decadal and long-term forecasts to quantify changes in species distribution.\nWe will be able to use these same decadal and long-term (to 2100) forecasts to quantify changes in important environmental covariates over time, to determine drivers of change.\n\n\n\nSpecies sensitivity will be estimated following similar methods to the original CVA. We will perform a literature review to update our understanding of important life history attributes and solict expert feedback on the most important life history attributes.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "content/methods-overview.html#overview",
    "href": "content/methods-overview.html#overview",
    "title": "Methods Overview",
    "section": "",
    "text": "The CVA2.0 will take a semi-quantiative approach, using both quantitative and qualitative methods.\n\n\nSpecies distributions will be quantified using ensemble species distribution models, forced by environmental covariates from the MOM6 hindcast (1993 - 2019) and driven by fisheries dependent and independent datasets. These species distribution models will then be projected into the future using the MOM6 decadal and long-term forecasts to quantify changes in species distribution.\nWe will be able to use these same decadal and long-term (to 2100) forecasts to quantify changes in important environmental covariates over time, to determine drivers of change.\n\n\n\nSpecies sensitivity will be estimated following similar methods to the original CVA. We will perform a literature review to update our understanding of important life history attributes and solict expert feedback on the most important life history attributes.",
    "crumbs": [
      "Methods"
    ]
  },
  {
    "objectID": "content/rpackage-overview.html",
    "href": "content/rpackage-overview.html",
    "title": "RPackage Overview",
    "section": "",
    "text": "An R package is undergoing active development to document the workflows and codes used to contruct the new Climate Vulnerability Assessment.\nThe package will have 4 major components:\n\nBuilding the ensemble species distribution models\nEstimating exposure\nSummarizing sensitivity from expert scores\nCalculating final vulnerability scores\n\nThe following pages provide a tutorial for the different portions of the package.",
    "crumbs": [
      "R Package"
    ]
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "The CVA2.0 project is funded by the Inflation Reduction Act.\nThis project would not have been possible without the dedication and hardwork of countless individuals aboard the federal and state-run survey programs and the NOAA Observer program.\nMany thanks to the following state-run programs for contributing data:\n\nNortheast Area Monitoring and Assessment Program Nearshore Trawl\nMaine-New Hampshire Inshore Trawl Survey\nMassachusetts Division of Marine Fisheries Bottom Trawl Survey\nLong Island Sound Trawl Survey\nNew York Nearshore Trawl Survey\nNew Jersey Ocean Stock Assessment Survey\nDelaware Trawl Surveys\n\nWe also thank all of the NOAA staff, members of the New England and Mid Atlantic Fisheries Management Councils, the Atlantic States Marine Fisheries Commission, and our academic partners for their feedback and assistance in developing the methods described here.\nThis website is based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "content/vulnerability_directionality.html",
    "href": "content/vulnerability_directionality.html",
    "title": "Vulnerability & Directionality",
    "section": "",
    "text": "Vulnerability\nThe final vulnerability scores will be estimated using similar methods to Boyce et al 2022 and 2024 - a weighted average of sensitivity, exposure, and if used in this analysis, adaptive, scores based on variability across the study area. For attributes based on life history stages, such as sensitivity and/or adaptability, these will be uniform across the study region. These averages will occur across space.\nThe final product will be a map of species vulnerability across the study region. An overall vulnerability score may be generated based on habitat suitability.\n\n\nDirectionality\nIn the original CVA, in addition to ranking how sensitive species may be to future climate change, experts also estimated if species would be positively or negatively impacted by climate change. For the CVA2.0, we will use the species distribution models to estimate current area of preferred habitat. We will calculate change in habitat area over time from the decadal and long-term projections from MOM6. We may also calculate directionality via expert opinion and compare the two metrics.",
    "crumbs": [
      "Methods",
      "Vulnerability & Directionality"
    ]
  },
  {
    "objectID": "content/exposure-calculation.html",
    "href": "content/exposure-calculation.html",
    "title": "Calculating Exposure",
    "section": "",
    "text": "Exposure is calculated from MOM6 forecast data using the same methods to calculate and rank raw exposure as the CVA1.0 on the MOM6 environmental data. We then use the predicted species distributions and the relative importance of covariates in the ensemble models to calculate variable-specific and total exposure for each species.",
    "crumbs": [
      "R Package",
      "Calculating Exposure"
    ]
  },
  {
    "objectID": "content/exposure-calculation.html#overview",
    "href": "content/exposure-calculation.html#overview",
    "title": "Calculating Exposure",
    "section": "",
    "text": "Exposure is calculated from MOM6 forecast data using the same methods to calculate and rank raw exposure as the CVA1.0 on the MOM6 environmental data. We then use the predicted species distributions and the relative importance of covariates in the ensemble models to calculate variable-specific and total exposure for each species.",
    "crumbs": [
      "R Package",
      "Calculating Exposure"
    ]
  },
  {
    "objectID": "content/exposure-calculation.html#calculating-and-ranking-raw-exposure",
    "href": "content/exposure-calculation.html#calculating-and-ranking-raw-exposure",
    "title": "Calculating Exposure",
    "section": "Calculating and Ranking Raw Exposure",
    "text": "Calculating and Ranking Raw Exposure\nAs in the CVA1.0, raw exposure for each environmental variable is calculated using a z-score, which is the difference in mean conditions in a future and present timeseries divided by present day standard deviations.\nThe function calcExposure will take the output from pull_hind and/or pull_forecast (a list of rasterStacks) and calculate monthly raw exposure for each environmental variable in the list:\n\nexposure &lt;- calcExposure(presentRasts, futureRasts)\n\nWhere presentRasts and futureRasts are lists of rasterStacks with the same length. These are the raw timeseries over the desired time periods. Remember that pull_hind and pull_forecast pull the complete timeseries for the desired MOM6 run, so you will need to subset their outputs accordingly if desired. calcExposure will produce a list of rasterStacks with the same length as the provided lists. Each rasterStack will have 12 layers, one for each month. \nThe next step is to rank the raw exposure. Since the CVA is designed to help determine which species are at risk to the greatest exposure, these ranks are based on this goal, rather than the distribution of the data themselves. We use the same ranks as the CVA1.0:\n\nRaw Exposure Ranks\n\n\nScore\nRaw Exposure\nRank\n\n\n\n\nLow\n&lt; 0.5\n1\n\n\nMedium\n0.5 - 1.5\n2\n\n\nHigh\n1.5 - 2\n3\n\n\nVery High\n&gt; 2\n4\n\n\n\nThe function rankExposure performs the ranking. It also provides the option to multiply raw exposure values for certain by -1 (aka flip). This option was included because the direction of change and the meaning of that change may differ across environmental variables. For example, an increase in surface temperatures would have a positive exposure, whereas a decrease in pH would have a negative exposure; both changes are considered bad for the environment, but the signs differ due to how these variables are quantified. The flip and noFlipList options provide the option to flip the signs of raw exposure for all variables except those listed in noFlipList.\n\nexpRanked &lt;- rankExposure(exposure, flip = T, noflipList = c('bottomT', 'surfaceT', 'bottomArg', 'MLD'))\n\n\n\n\nBottom Temperature Ranked Exposure comparing 2019-2019 and 2025-2035",
    "crumbs": [
      "R Package",
      "Calculating Exposure"
    ]
  },
  {
    "objectID": "content/exposure-calculation.html#creating-species-specific-exposure-time-series-and-maps-for-each-variable",
    "href": "content/exposure-calculation.html#creating-species-specific-exposure-time-series-and-maps-for-each-variable",
    "title": "Calculating Exposure",
    "section": "Creating Species-Specific Exposure Time Series and Maps for Each Variable",
    "text": "Creating Species-Specific Exposure Time Series and Maps for Each Variable\nOnce raw exposure is ranked, species-specific exposure for each environmental variable can be calculated using the functions makeExposureTimeseries and makeExposureMaps. These functions are nearly identical and only require the ranked exposure values and the mean ensemble species distribution model (SDM) predictions. Ranked exposure values are averaged across space within each month in makeExposureTimeseries and averaged across months in each model grid cell in makeExposureMaps, using the SDM results as weights to account for species distributions and habitat use. The results are a time series of average ranked species-specific exposure within each month and a raster of average species-specific exposure across time for each environmental variable.\n\n#map\nmapExp &lt;- makeExposureMaps(rankExp = expRanked, sdmRast = avgHSM)\n\n#timeseries\nvecExp &lt;- makeExposureTimeseries(rankExp = expRanked, sdmRast = avgHSM)\n\nThe ensemble SDM results produced by make_predictions should be converted into a monthly average prior to using makeExposureTimeseries and makeExposureMaps. It is also recommended that you remove low SDM values after averaging. In the NE Shelf CVA2.0, we removed all SDM values less than 0.1 (representing areas with less than a 10% likelihood of finding the species). Not removing these low values resulted in higher than expected exposure results in areas where the species is rare.\nThe output from makeExposureTimeseries is a matrix with 12 columns (1 for each month) and a number of rows equal to the number of environmental variables in expRanked. makeExposureMaps produces a rasterStack, with each layer representing one of the environmental variables. Values should range from 1 to 4.\nThe wrapper function makeVariableAverages is provided and can be run in parallel using the furrr package, similar to the SDM wrapper functions. Like those wrapper functions, your directory must be set up according to the Installation and Setup Guide provided for them to function. makeVariableAverages also generates the monthly average SDM results, and removes low values for you.\n\nplan(multisession, workers = 6)\nchecks &lt;- future_pmap(list(..1 = args$spp, ..2 = args$ensName, ..3 = args$pStart, ..4 = args$pEnd, ..5 = args$fStart, ..6 = args$fEnd), ~ makeVariableAverages(spp = ..1, ensName = ..2, pStart = ..3, pEnd = ..4, fStart = ..5, fEnd = ..6), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)\n\nLike the SDM wrapper functions, args is a dataframe where each row is a set of arguments to be passed to the function. Here, the arguments include the species name (spp), ensemble name to help locate the appropriate ensemble model (ensName), and start and end years for the present and future timeseries (pStart/pEnd/fStart/fEnd) to load in the correct exposure values, as well as save the results from makeExposureTimeseries and makeExposureMaps appropriately. These are not memory or time-intensive calculations, so it is not necessary to run them in parallel - it is just helpful if you have a lot of species to work through.\nBelow are example variable-specific exposures for Atlantic cod: \n\n\n\nVariable-specific exposures across space",
    "crumbs": [
      "R Package",
      "Calculating Exposure"
    ]
  },
  {
    "objectID": "content/exposure-calculation.html#calculating-total-exposure-for-each-species",
    "href": "content/exposure-calculation.html#calculating-total-exposure-for-each-species",
    "title": "Calculating Exposure",
    "section": "Calculating Total Exposure for Each Species",
    "text": "Calculating Total Exposure for Each Species\nOnce you have the species-specific exposure calculated for each variable, they must be combined to estimate total species-specific exposure. These will be the final spatially and temporally explicit values that are combined with species sensitivity to estimate vulnerability.\nTo combine the species-specific exposures for each variable, we use the same logic rule as the CVA1.0:\n\nLogic Rule used to Calculate Total Species Exposure\n\n\nOverall Score\nNumeric Score\nLogic Rule\n\n\n\n\nLow\n1\nAll other scores\n\n\nMedium\n2\n2 or more attributes &gt;= 2.5\n\n\nHigh\n3\n2 or more attributes &gt;= 3.0\n\n\nVery High\n4\n3 or more attributes &gt;= 3.5\n\n\n\nWe calculate total exposure in two ways: 1) Applying the logic rule to all ecologically relevant environmental variables as defined by the feeding and habitat keys and 2) Only applying the logic rule to ecological variables that have relative weights in the final ensemble SDM greater than a set value (for the NE Shelf CVA2.0, we chose 10% or 0.1). This was done to account for possible unknown changes in variable importance under future climate change. The latter method (only counting the most important variables), assumes that these values will continue to be very important under future climate change scenarios. It is unclear if this will be the case, especially for variables such as pH where it is unclear how they will impact species. Calculating total exposure using all the ecologically relevant variables allows us and stakeholders to consider all variables regardless of their model importance.\nThe functions combineTimeseries and combineMaps provide options for both methods with the option countAll. Setting countAll to TRUE will automatically count all provided variables in matExp for combineTimeseries or mapExp for combineMaps, which are the outputs from makeExposureTimeseries and makeExposureMaps, respectively. If set to FALSE, you will need to provide the relative weights and the threshold to use to count variables in the logic rule. It is important that, if you have not yet done so, that you subset matExp and mapExp to only the ecologically relevant variables before running the combineTimeseries and combineMaps functions. This can be done using the relative weights of the variables.\nThe function combineWeights estimates the relative variable importance across the models. Since every component model of the ensemble SDM estimates variable importance differently, all importance data is normalized to their respective sums such that they total 1. A weighted average is then performed across models using the weights of each component model in the ensemble. The result is a vector of values representing the relative importance of each of the ecologically relevant variables to the model.\n\ncW &lt;- combineWeights(vars = vars, ensWeights = weights, impFlist = iFlist, staticNames = staticVars)\n\nWhere vars is a vector of character strings including the ecologically relevant variables, ensWeights is the weights of each component model of the ensemble model generated by make_sdm, impFlist is a vector of character strings including the file paths to the model variable importance files made by sdm_importance, and staticNames are the names of any static environmental variables to be excluded.\nThe names of the vector resulting from combineWeights can be used to subset the outputs from makeExposureTimeseries and makeExposureMaps, since these output contain all possible environmental variables from MOM6, before running combineTimeseries and combineMaps.\n\n#subset timeseries matrix by rownames\n  i &lt;- rownames(vecExp) %in% names(cW)\n  vecSub &lt;- vecExp[i,]\n\n#timeseries\n  totalTA &lt;- combineTimeseries(matExp = vecSub, weights = cW, wThreshold = 0, countAll = T) #counting all variables\n  totalTS &lt;- combineTimeseries(matExp = vecSub, weights = cW, wThreshold = 0.1, countAll = F) #counting only ecologically relevant variables\n\n#subset mapExp by rownames\n  i &lt;- names(mapExp) %in% names(cW)\n  mapSub &lt;- raster::subset(mapExp, which(i == T))\n#maps\n  totalM &lt;- combineMaps(mapExp = mapSub, weights = cW, wThreshold = 0, countAll = T)\n  totalMS &lt;- combineMaps(mapExp = mapSub, weights = cW, wThreshold = 0.1, countAll = F)\n\nBelow are examples of total exposure using both methods for Atlantic cod: \n\n\n\nTotal Exposure using only important variables across time\n\n\n\n\n\nTotal Exposure using all ecologically-relevant variables across space\n\n\n\n\n\nTotal Exposure using only important variables across space",
    "crumbs": [
      "R Package",
      "Calculating Exposure"
    ]
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/install-setup.html",
    "href": "content/install-setup.html",
    "title": "Installation & Setup",
    "section": "",
    "text": "Installation instructions will come once the R package is finalized.",
    "crumbs": [
      "R Package",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "content/install-setup.html#installation",
    "href": "content/install-setup.html#installation",
    "title": "Installation & Setup",
    "section": "",
    "text": "Installation instructions will come once the R package is finalized.",
    "crumbs": [
      "R Package",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "content/install-setup.html#set-up",
    "href": "content/install-setup.html#set-up",
    "title": "Installation & Setup",
    "section": "Set Up",
    "text": "Set Up\n\nNecessary Files\n\nKey Files\nIf you wish to subset environmental covariates used in the models when making the data frames, you will need files that 1) list the species and their habitat and feeding guilds and 2) define the covariates to keep for each feeding and habitat guild, referred to as key files.\nThe species list should at least include a column with the species name and then columns for the associated feeding and habitat guild. While it is not necessary, this file is also a good place to list alternative species names (common names, scientific names, any variations on either) that can be used to make sure that observations of the same species are combined across data types.\n\nspp &lt;- read.csv('spp_list.csv')\nhead(spp)\n\n           Common.Name             COM_NAME           Scientific.Name\n1        Atlantic cod         ATLANTIC COD               Gadus morhua\n2     Atlantic croaker     ATLANTIC CROAKER   Micropogonias undulatus\n3     Atlantic halibut     ATLANTIC HALIBUT Hippoglossus hippoglossus\n4     Atlantic herring     ATLANTIC HERRING           Clupea harengus\n5    Atlantic mackerel    ATLANTIC MACKEREL          Scomber scombrus\n6 Atlantic sea scallop ATLANTIC SEA SCALLOP  Placopecten magellanicus\n     Alternate.Name                  SCI_NAME            SCI_NAME_ALT\n1      Cod Atlantic              GADUS MORHUA                        \n2  CROAKER ATLANTIC   MICROPOGONIAS UNDULATUS MICROPOGONIAS UNDULATES\n3  Halibut Atlantic HIPPOGLOSSUS HIPPOGLOSSUS        HALIBUT ATLANTIC\n4  Herring Atlantic           CLUPEA HARENGUS                        \n5 Mackerel Atlantic          SCOMBER SCOMBRUS       MACKEREL ATLANTIC\n6       Scallop Sea  PLACOPECTEN MAGELLANICUS PLACOPECTEN MAGELANICUS\n            SCI_NAME_ALT2 Managing.Body Feeding.Guild Habitat.Guild\n1                                 NEFMC     Piscivore    Groundfish\n2 MICROPOGONIUS UNDULATUS         ASMFC    Benthivore    Groundfish\n3                                 NEFMC     Piscivore    Groundfish\n4                                 NEFMC   Planktivore       Pelagic\n5                                 MAFMC   Planktivore      Pelagic \n6                                 NEFMC       Benthos       Benthic\n\n\nKey files should have seperate columns, for each feeding and habitat guild with names that match the different feeding and habitat guilds in the species list. The entries in each column should be a column name associated with that guild.\n\nfeed &lt;- read.csv('feeding_guilds.csv')\nhead(feed)\n\n  Planktivore Piscivore Benthos Benthivore Apex.Predator\n1      diazPP  smallZoo  intNPP     intNPP        intNPP\n2     smallPP mediumZoo     POC        POC              \n3    mediumPP  largeZoo                                 \n4     largePP                                           \n\n\n\nhab &lt;- read.csv('habitat_guilds.csv')\nhead(hab)\n\n  Groundfish   Benthic   Pelagic Pelagic.Migratory\n1    bottomT   bottomT  surfaceT          surfaceT\n2    bottomS   bottomS  surfaceS          surfaceS\n3   bottomO2  bottomO2 surfacepH         surfacepH\n4  bottomArg bottomArg       MLD               MLD\n\n\nIt is critical that the column names in the key files and the different habitat/feeding guild names in the species key are identical (remember: ‘Pelagic’ and ‘Pelagic’ are technically two different character strings in R) or else they will not be matched correctly.\n\n\nFisheries Data\nThis workflow is designed to accept both fisheries independent and dependent data sources. Data standardization functions have the ability to pull NEFSC survey (using survdat) and observer data using ROracle, but can also accept CSV files. CSV files will need records of all efforts, regardless of if the target species is collected or not, to correctly document absences and have a time column that can be converted with the POSIXct function.\n\n\nEnvironmental Data\nFunctions are provided to pull hindcast or forecast data from MOM6 models via the CEFI portal. Other environmental data can be supplied, but should be on regular grids and are ideally netcdf files that can be read using raster if fisheries presence/absence rasters are desired on the same grid.\n\n\n\nDirectory\nThis R package is designed to work within a single working directory with folders for the Exposure and SDM results. To use the wrapper functions provided, the working directory should include a folder for each target species, a Data folder, and a logs folder. Key files can be added to the SDM working directory.\nWithin the SDM directory, the Data subdirectory should have subfolders for csv and MOM6 (or other environmental) data, as well as an object containing a list of static environmental covariates (bathymetry, distance to shore, etc). The CSV folder should contain raw and standardized subfolders to hold the raw csv files from different data sources, and their standardized counterparts from the standardize_data function. The MOM6 data should contain the raw, averaged, standard deviation, and normalized outputs from the MOM6 functions. Each species folder should contain three subfolders: 1) input_rasters, 2) model_output, and 3) output_rasters. The input_rasters folder contains all the individual rasters from each data source and the combined raster. The output_rasters folder contains all the predicted rasters from each model and the final ensemble model. The model_output folder contains the following folders: 1) models, 2) cvs, 3) preds, 4) eval_metrics, and 5) importance. Each of these folders will contain the output from their respective functions for each model component, and as necessary, the ensemble model.\nThe Exposure directory has a similar structure to the SDM directory, where each species specific folder contains seperate folders for Data and Figures. It is recommended that you use seperate subfolders within each of these if you are calculating exposure across multiple time frames to keep them seperate. The RawExposure folder contains the raw and ranked exposure data and figures.\n\n\n\nExample directory",
    "crumbs": [
      "R Package",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "",
    "text": "The Climate Vulnerability Assessment (CVA) was originally performed by the Northeast Fisheries Science Center in 2015. This original work, which was later replicated by all the National Marine Fisheries Service (NMFS) regions, used expert opinions to rank how target species would be impacted by future climate change using both exposure and sensitivity attributes. For the original CVA in the northeast US, experts ranked how much change species would be exposed to using large glocal ocean model forecasts and how sensitive they would be to these changes. This resulted in a single exposure and sensitivity score for each species, which was combined to give an overall vulnerability score. Experts were also asked to rank the directionality of climate impacts. The original methods are described in Morrison et al 2015 and Hare et al 2016, and the original results for the northeast shelf can be found in Hare et al 2016. CVA results for all regions can be explored using the NOAA CVA Tool.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#history-of-the-climate-vulnerability-assessment",
    "href": "index.html#history-of-the-climate-vulnerability-assessment",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "",
    "text": "The Climate Vulnerability Assessment (CVA) was originally performed by the Northeast Fisheries Science Center in 2015. This original work, which was later replicated by all the National Marine Fisheries Service (NMFS) regions, used expert opinions to rank how target species would be impacted by future climate change using both exposure and sensitivity attributes. For the original CVA in the northeast US, experts ranked how much change species would be exposed to using large glocal ocean model forecasts and how sensitive they would be to these changes. This resulted in a single exposure and sensitivity score for each species, which was combined to give an overall vulnerability score. Experts were also asked to rank the directionality of climate impacts. The original methods are described in Morrison et al 2015 and Hare et al 2016, and the original results for the northeast shelf can be found in Hare et al 2016. CVA results for all regions can be explored using the NOAA CVA Tool.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#why-cva2.0",
    "href": "index.html#why-cva2.0",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "Why CVA2.0?",
    "text": "Why CVA2.0?\nThe original CVA produced single metrics for each species across their entire range. However, in the past decade, it has become clear that climate change is not uniform worldwide. Therefore, species or stocks may be more or less vulnerable in different portions of their range. As a result, there is a need for a spatially explicit CVA. In addition, the development of high resolution regional ocean models such as the MOM6 model also allow changes to be predicted on fine spatial and temporal scales. The availability of the MOM6 hindcast and forecast on both decadal and long-term scales, plus new information on life history attributes for species, will allow us to update the existing CVA.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Climate Vulnerability Assessment 2.0",
    "section": "How to Use this Book",
    "text": "How to Use this Book\nThe purpose of this book is to document the methods and capture the decisions made in the development of the CVA2.0 for the Northeast Shelf, so that these methods could be broadly applied to other NMFS regions or management bodies. This book will also outline the R package that is actively in development to assist in reproducability across regions. Automatic reporting methods and any future RShiny application development to assist in gathering expert opinons will also be outlined here.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#quarto-documentation",
    "href": "content/customizing.html#quarto-documentation",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "Examples",
    "text": "Examples\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFay lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  },
  {
    "objectID": "content/data.html",
    "href": "content/data.html",
    "title": "Data",
    "section": "",
    "text": "We utilized two different data types for this work - fisheries independent and dependent data.\n\n\nFisheries independent data comes from sources outside of the fishing industry. These often come in the form of regular surveys run by federal or state-level management bodies. Traditionally, these surveys occur in the spring and fall. For the CVA2.0 in the Northeast, we used the following surveys:\n\nNOAA Northeast Ecosystems Surveys\nNortheast Area Monitoring and Assessment Program Nearshore Trawl\nMaine-New Hampshire Inshore Trawl Survey\nMassachusetts Division of Marine Fisheries Bottom Trawl Survey\nLong Island Sound Trawl Survey\nNew York Nearshore Trawl Survey\nNew Jersey Ocean Stock Assessment Survey\nDelaware Trawl Surveys\n\n\n\n\nFisheries dependent data is data collected by commercial and recreational fishers. In the United States, commercial fisheries data is collected by fisheries observers in the National Observer Program, Gillnet Observer Program, and Pelagic Observer Program. These are trained individuals who accompany fishing boats on their trips to collect data on the species caught. Data from recreational fisheries is collected using post-effort in person or phone surveys at different docks via the logbook and Large Pelagics Survey programs.\n\n\n\nFisheries independent and dependent data are collected using different methods - they could fish for different lengths of time, at different speeds, or use different equipment. This is especially true for fisheries dependent data, which may used specialized equipment depending on the species being targeted. Therefore, we cannot compare the number of individuals caught across these methods. As a result, we will only examine whether species are present or absent and will not model the number, or abundance, of species. While different fishing methods could impact the ability to catch a particular species, thus impacting our results, we believe that this approach will account for most differences and will assume that all datasets are sampling for long enough periods of time and appropriate equipment to sample all species.",
    "crumbs": [
      "Methods",
      "Data"
    ]
  },
  {
    "objectID": "content/data.html#fisheries-data",
    "href": "content/data.html#fisheries-data",
    "title": "Data",
    "section": "",
    "text": "We utilized two different data types for this work - fisheries independent and dependent data.\n\n\nFisheries independent data comes from sources outside of the fishing industry. These often come in the form of regular surveys run by federal or state-level management bodies. Traditionally, these surveys occur in the spring and fall. For the CVA2.0 in the Northeast, we used the following surveys:\n\nNOAA Northeast Ecosystems Surveys\nNortheast Area Monitoring and Assessment Program Nearshore Trawl\nMaine-New Hampshire Inshore Trawl Survey\nMassachusetts Division of Marine Fisheries Bottom Trawl Survey\nLong Island Sound Trawl Survey\nNew York Nearshore Trawl Survey\nNew Jersey Ocean Stock Assessment Survey\nDelaware Trawl Surveys\n\n\n\n\nFisheries dependent data is data collected by commercial and recreational fishers. In the United States, commercial fisheries data is collected by fisheries observers in the National Observer Program, Gillnet Observer Program, and Pelagic Observer Program. These are trained individuals who accompany fishing boats on their trips to collect data on the species caught. Data from recreational fisheries is collected using post-effort in person or phone surveys at different docks via the logbook and Large Pelagics Survey programs.\n\n\n\nFisheries independent and dependent data are collected using different methods - they could fish for different lengths of time, at different speeds, or use different equipment. This is especially true for fisheries dependent data, which may used specialized equipment depending on the species being targeted. Therefore, we cannot compare the number of individuals caught across these methods. As a result, we will only examine whether species are present or absent and will not model the number, or abundance, of species. While different fishing methods could impact the ability to catch a particular species, thus impacting our results, we believe that this approach will account for most differences and will assume that all datasets are sampling for long enough periods of time and appropriate equipment to sample all species.",
    "crumbs": [
      "Methods",
      "Data"
    ]
  },
  {
    "objectID": "content/data.html#environmental-data",
    "href": "content/data.html#environmental-data",
    "title": "Data",
    "section": "Environmental Data",
    "text": "Environmental Data\nWe will be using species distribution modeling to quantify the relationships between species presence and their environment. We are using the Modular Ocean Model (MOM) 6 hindcast data to drive these models. The MOM6 is developed by the Changing Fisheries and Ecosystem Initiative and includes both a physical ocean model and a coupled biogeochemical model.\n\nFor the CVA2.0\nWe are using the hindcast timeseries from 1993-2019 from the MOM6 for the Northwest Atlantic to build the species distribution models. We will then project these models into the future using the decadal and long-term projections from the MOM6.\nCovariates used to build the species distribution models include:\n\nSea Surface Temperature & Salinity\nBottom Temperature & Salinity\nSurface pH\nBottom Aragonite Solubility\nMixed Layer Depth\nDiazotroph, small, medium, and large phytoplankton primary productivity (integrated in the top 100 m)\nSmall, medium, and large zooplankton biomass (integrated in the top 100 m)\nDownward particulate organic carbon (POC) flux\nWater column net primary productivity (NPP)\n\nCovariates will be normalized to long term (1993 - 2019) averages and matched to species based on their habitat and prey preferences. For example, the species distribution model for summer flounder will be built using bottom temperature and salinity, bottom aragonite solubility, downward POC flux, and NPP.",
    "crumbs": [
      "Methods",
      "Data"
    ]
  },
  {
    "objectID": "content/add-content.html",
    "href": "content/add-content.html",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#edit-and-add-your-pages",
    "href": "content/add-content.html#edit-and-add-your-pages",
    "title": "Customize",
    "section": "",
    "text": "Edit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#add-your-pages-the-project",
    "href": "content/add-content.html#add-your-pages-the-project",
    "title": "Customize",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "content/ensemble-sdms.html",
    "href": "content/ensemble-sdms.html",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "",
    "text": "The first step in the CVA2.0 workflow is to build the species distribution models. These will be the basis of the exposure, directionality, and possibly additional indicators from the CVA. These are what allow the calculations to be spatially explicit and account for species habitat use.\nWithin this workflow, there are three steps:\n\nData preparation\nBuilding & Predicting Species Distribution Models\nBuilding & Predicting the Ensemble\n\nThe functions outlined below are designed to be run either sequentially or in parallel with the R parallelization method of your choice (doParallel and furrr are recommended) and can be run in loops across a list of species.",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#overview",
    "href": "content/ensemble-sdms.html#overview",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "",
    "text": "The first step in the CVA2.0 workflow is to build the species distribution models. These will be the basis of the exposure, directionality, and possibly additional indicators from the CVA. These are what allow the calculations to be spatially explicit and account for species habitat use.\nWithin this workflow, there are three steps:\n\nData preparation\nBuilding & Predicting Species Distribution Models\nBuilding & Predicting the Ensemble\n\nThe functions outlined below are designed to be run either sequentially or in parallel with the R parallelization method of your choice (doParallel and furrr are recommended) and can be run in loops across a list of species.",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#data-preparation",
    "href": "content/ensemble-sdms.html#data-preparation",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nFisheries Data\nThe function standardize_data can be used to standardize provided csv files or pull NEFSC survey or observer data from NEFSC databases via ROracle. This can also be accomplished in R outside of the package, but standardize_data provides a single function to format data in a way that will be accepted by downstream functions. If you are pulling together data from a variety of sources, you will likely still need to manipulate data outside of this function to make sure that data are in a single csv file and that dates and times are in the correct format.\nTo pull NEFSC survey or observer data, a channel from the package dbutils must be provided:\n\nsurv &lt;- standardize_data(dataType = 'Surveys', channel = dbutils::connect_to_database(server=\"NEFSC_pw_oraprod\",uid=\"KGALLAGHER\"))\n\nTo standardize CSV data, a file path to the csv file, as well as column names for station IDs; position (longitude/latitude); date; the column corresponding to the presence/absence, or count of the species; and species name.\n\nneamap &lt;- standardize_data(dataType = 'CSV', csv = \"./Data/csvs/raw/NEAMAP_Tow_Catch_2025-08-14.csv\", csvCols = c('station', 'lon', 'lat', 'date', 'present_absent', 'SCI_NAME'))\n\nThis function results in a dataframe being transformed from this:\n\n\n  X      ID TRIP_NUMBER.x HAUL_NUMBER.x AREA_FISHED SET_BEGIN_DATE SET_END_DATE\n1 1 44473.1         44473             1        3674      4/11/2010    4/11/2010\n2 2 44473.2         44473             2        3674      4/11/2010    4/11/2010\n3 3 44473.3         44473             3        3674      4/11/2010    4/11/2010\n4 4 44473.5         44473             5        3674      4/11/2010    4/11/2010\n5 5 44473.5         44473             5        3674      4/11/2010    4/11/2010\n6 6 44473.5         44473             5        3674      4/11/2010    4/11/2010\n  HAUL_BEGIN_DATE HAUL_END_DATE SET_BEGIN_LAT_CONV SET_BEGIN_LONG_CONV\n1       4/11/2010     4/11/2010             36.068             -74.921\n2       4/11/2010     4/11/2010             36.050             -74.909\n3       4/11/2010     4/11/2010             36.042             -74.942\n4       4/11/2010     4/11/2010             36.002             -74.975\n5       4/11/2010     4/11/2010             36.002             -74.975\n6       4/11/2010     4/11/2010             36.002             -74.975\n  HAUL_BEGIN_LAT_CONV HAUL_BEGIN_LONG_CONV HAUL_END_LAT_CONV HAUL_END_LONG_CONV\n1              36.063               74.921            36.065             74.936\n2              36.049               74.909            36.050             74.913\n3              36.044               74.941            36.044             74.945\n4              36.007               74.975            36.004             74.975\n5              36.007               74.975            36.004             74.975\n6              36.007               74.975            36.004             74.975\n  SET_DURATION HAUL_DURATION_HOURS GEAR_SOAK_HOURS  startDate VESSEL_ID\n1         0.07                0.90            3.08 2010-04-11    926397\n2         0.07                0.32            0.80 2010-04-11    926397\n3         0.03                0.18            0.48 2010-04-11    926397\n4         0.05                0.27            0.50 2010-04-11    926397\n5         0.05                0.27            0.50 2010-04-11    926397\n6         0.05                0.27            0.50 2010-04-11    926397\n  TRIP_NUMBER.y HAUL_NUMBER.y         SPECIES_NAME NUM_FISH SPECIES_ON_LIST\n1         44473             1             BLUEFISH      363               1\n2         44473             2             BLUEFISH       64               1\n3         44473             3             BLUEFISH        7               1\n4         44473             5             BLUEFISH       14               1\n5         44473             5 SHARK DOGFISH SMOOTH        3               0\n6         44473             5  SHARK DOGFISH SPINY        6               0\n\n\nTo this:\n\n\n  X       time month year   towID     lon    lat       date count\n1 1 2010-04-11     4 2010 44473.1 -74.921 36.068 2010-04-11   363\n2 2 2010-04-11     4 2010 44473.2 -74.909 36.050 2010-04-11    64\n3 3 2010-04-11     4 2010 44473.3 -74.942 36.042 2010-04-11     7\n4 4 2010-04-11     4 2010 44473.5 -74.975 36.002 2010-04-11    14\n5 5 2010-04-11     4 2010 44473.5 -74.975 36.002 2010-04-11     3\n6 6 2010-04-11     4 2010 44473.5 -74.975 36.002 2010-04-11     6\n                  name\n1             BLUEFISH\n2             BLUEFISH\n3             BLUEFISH\n4             BLUEFISH\n5 SHARK DOGFISH SMOOTH\n6  SHARK DOGFISH SPINY\n\n\nTo ensure that fisheries and environmental data are on the same scale, and to convert any abundance data to presence/absence, the fisheries csvs produced by standardize_data are then converted into rasters of effort/presence/absence with the function create_rast. While the source datasets are generated for the entire dataset, one rasterStack is produced for each target species and data source. create_rast produces rasters with the same resolution as the environmental data where values range from 0 to 2, where 0 means that no effort was present in the grid cell, 1 means that there was fishing effort but the target species was not caught, and 2 indicates that the target species was caught in that grid cell.\nUsers must provide the standardized csv files generated by standardize_data, a link to a netcdf file that the desired grid and timesteps can be extracted from, the start date of the timeseries, and a vector of alternate names for the target species to account for any differences in species names across data sources. The user must also indicate if the data come from fisheries independent (surveys) or dependent sources (observer or logbook programs). If data come from fisheries dependent sources, then a threshold is applied before the raster is created, following McHenry et al 2019; rasters are only created if the species is present at least 30 times throughout the dataset.\n\nrast &lt;- create_rast(data = csv, isObs = FALSE, grid = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/regrid/r20250715/tob.nwa.full.hcast.monthly.regrid.r20250715.199301-202312.nc\", origin = '1993-01-01', targetVec = \"Atlantic cod ,ATLANTIC COD ,Gadus morhua,Cod Atlantic,GADUS MORHUA\") #example to make raster for Atlantic Cod\n\nThe resulting rasterStack will have a range from 0-2 with a number of layers equal to the length of the timeseries. Below is an example layer from the produced rasterStack for Atlantic Cod from the NEFSC survey data. \nThe wrapper function saveRast is provided for create_rast. This wrapper function provides logging functionality to keep track of progress, which is especially helpful if running the code in parallel, and also adds a skip functionality to skip making the raster if it already exists for that species and data source. It also saves the resulting rasterStack in the input_rasters directory within the species-specific folder in the working directory.\nHere is an example of using saveRast in parallel using the package furrr with future_pmap:\n\nplan(multisession, workers = 5)\nfuture_pmap(list(..1 = argsList$csvName, ..2 = argsList$spp, ..3 = altNames, ..4 = argsList$skip, ..5 = argsList$isObs), ~ saveRast(csvName = ..1, spp = ..2, sppNames = ..3, skip = ..4, isObs = ..5, grid = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/regrid/r20230520/tob.nwa.full.hcast.monthly.regrid.r20230520.199301-201912.nc\", origin = '1993-01-01'), .progress = T)\nplan(sequential)\n\nWhere argsList is a dataframe with a row for each desired call of saveRast, and a column associated with most arguments for saveRast and create_rast. This is generated using tidyr::expand_grid to create a dataframe of every combination of target species and data sources.\ncreate_rast creates a seperate rasterStack for each species, which is saved to the species-specific input_rasters folder bv the saveRast wrapper function. This allows the user to examine the raster for each data source independently to ensure that it meets expectations. Before these can be matched with the environmental data, the rasters must be combined into a single rasterStack, while maintaining the 0-2 range to document presences, absences, and lack of effort. This is done with the merge_rasts and associated combineSave wrapper function. The combineSave wrapper function, in addition to providing logging and skip functionality, also automatically creates a list of raster objects in the input_rasters folder for the merge_rasts function.\n\ncombinedRasts &lt;- merge_rasts(rasts)\n#where rasts is a list of rasterStacks to combine\n\nThe wrapper function combineSave is similar to saveRast where is provides logging and skip functionality. This function also handles creating the list of rasterStacks necessary to provide to create_rast. It is also written to be run in parallel:\n\noptions(future.globals.maxSize = Inf)\nplan(multisession, workers = 5)\ncombs &lt;- future_pmap(list(..1 = args$name, ..2 = args$skip), ~ combineSave(name = ..1, skip = ..2), .progress = T)\nplan(sequential)\n\nThe resulting combined rasterStack will retain the range from 0-2 with a number of layers equal to the length of the timeseries. It works by retaining the maximum value in a grid cell across a given layer (which represent different timestamps) of the provided rasterStacks. Below is an example layer from the merged rasterStack for Atlantic Cod from the NEFSC survey data. \n\n\nEnvironmental Data\nThe example below illustrates how to grab and normalize monthly data from the Modular Ocean Model (MOM6) from the Northeast Shelf. The pull_hind and pull_forecast functions are designed to work with any MOM6 output. Other environmental data can be used as well, and normalized using the same functions, as long as the raw data is in the expected format.\nThe first step in getting the environmental data is pulling and spatially subsetting the raw MOM6 data. The functions provided default to pulling the entire time series of the specified model release. Functionality to subset the data by time may be added at a later date, as the length of the timeseries impacts how long the functions take the spatially subset the data.\nThe function pull_hind is designed to pull data from the hindcast simulations:\n\n r &lt;- pull_hind(jsonURL = \"https://psl.noaa.gov/cefi_portal/data_index/cefi_data_indexing.Projects.CEFI.regional_mom6.cefi_portal.northwest_atlantic.full_domain.hindcast.json\", reqVars = 'Bottom Temperature', shortNames = 'bottomT', release = 'r20230520', gt = 'regrid', of = 'monthly', bounds = c(-78,-65, 35,45), static = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/raw/r20230520/ocean_static.nc\")\n\nThe function ‘pull_forecast’ pulls data from the specified forecast similations:\n\n r &lt;- pull_forecast(jsonURL = \"https://psl.noaa.gov/cefi_portal/data_index/cefi_data_indexing.Projects.CEFI.regional_mom6.cefi_portal.northwest_atlantic.full_domain.decadal_forecast.json\", reqVars = 'Sea Water Potential Temperature at Sea Floor', shortNames = 'bottomT', release = 'r20250925', init = 'i202501', ens = 1, gt = 'regrid', of = 'monthly', bounds = c(-78,-65, 35,45), static = \"http://psl.noaa.gov/thredds/dodsC/Projects/CEFI/regional_mom6/cefi_portal/northwest_atlantic/full_domain/hindcast/monthly/raw/r20230520/ocean_static.nc\")\n\nNote that pull_forecast has the additional arguments ‘init’ and ‘ens’, which represent the initalization date and the ensemble member desired. As of October 2025, the seasonal and decadal forecasts from MOM6 include 10 ensembles with slightly different initial forcing parameters, and as a result can be averaged across ensembles if desired, whereas the long-term forecasts have ensembles that represent different projected CO2 and climate conditions.\nFor either function, the JSON file link can be found on the CEFI portal in the ‘Data Access’ tab. The arguments reqVars, release, gt, of, init, and ens help specify which variables to pull, at what output frequency, and what grid to pull (raw vs regrid; the latter is recommended). These need to match their corresponding columns in the JSON file exactly. See the documentation for these functions for more details.\nSince spatially subsetting the data can take some time (upwards of 30 minutes per variable), if you are pulling a lot of environmental variables, it is recommended to run this command in parallel. Generally, this is not a memory-intensive activity; it just takes some time depending on the length of the timeseries (AKA the number of layers in the resulting rasterBrick). Below is an example of running the pull_hind function in parallel with doParallel:\n\ncluster &lt;- makeCluster(10, type='PSOCK')\nregisterDoParallel(cluster)\nraw &lt;- foreach(x = 1:nrow(var.list), .packages = c(\"ncdf4\", 'raster', 'jsonlite')) %dopar% {\n  r &lt;- pull_hind(varURL = \"https://psl.noaa.gov/cefi_portal/data_index/cefi_data_indexing.Projects.CEFI.regional_mom6.cefi_portal.northwest_atlantic.full_domain.hindcast.json\", reqVars = var.list$Long.Name[x], shortNames = var.list$Short.Name[x], release = 'r20230520')\n\n  raw[[x]] &lt;- r\n}\nstopCluster(cluster)\nnames(raw) &lt;- var.list$Short.Name\n\nWhere var.list is a two column data frame with the full names of the desired variable (Long.Name) and an abbreviated variable name (Short.Name) which will eventually become the column name in the final dataframe.\nOnce the raw hindcast or forecast data is pulled, the workflow moving forward is the same: mean and standard deviations are calculated monthly to calculate z-scores for the complete timeseries. It is recommended to normalize the data in some way prior to building the models to help with interpretation since all the environmental data will likely have different units. We chose to normalize using a z-score to be consistent with exposure calculation methods.\n\n# r is a list of rasterStacks of raw bottom temperature, bottom salinity, and bottom oxygen data \na &lt;- avg_env(r) #calculate monthly average\ns &lt;- sd_env(r) #calculate monthly standard deviation\nn &lt;- norm_env(rawList = r, avgList = a, sdList = s, shortNames = c('bottomT', 'bottomS', 'bottomO2')) #calculate z-score for every time step\n\nYou could also perform each of these steps in parallel following the example above. However, it is not really necessary since these steps are faster than the initial data pull. That being said, it is important to note that the avg_env, sd_env, and norm_env functions expect lists of rasterStacks (which would be the outcome of running pull_hind or pull_forecast in parallel as illustrated above).\nUnlike other functions in this package, no wrapper function is provided. It is not shown here but the resulting lists should be stored in the ./Data/MOM6/ folder so that the data pull only needs to be performed once; it is also recommended to save the raw, mean, and standard deviations, even if they aren’t used in the models. This will allow you to 1) easily examine the raw data and 2) calculate z-scores relative to different time series if you wish to do so (for example, normalize forecast data to the present-day mean and standard deviations).\nIf you have static environmental data such as bathymetry or distance to shore, these should also be normalized to their respective means and standard deviations. You should create an R object that is a named list of your static variables and put it in the ./Data/MOM6/ folder. Names will correspond to the columns in the data frame if you choose to include these data in your final data frame. Cropping these rasters to the same extent as the MOM6 data is helpful for managing file size. We do not provide functions for this process as these data can come from a variety of sources.\n\n\nCreating Data Frames\nAt this point, you should have two sets of raster files: one containing the fisheries effort/presence/absence data and another containing your environmental data. If you want to include static environmental covariates such as bathymetry, you will have a third set of raster files. While rasters are easy to plot, most of our ensemble model members do not take rasters as inputs, so these need to be converted to data frames.\nThe function merge_spp_env is provided to merge species and environmental rasters together, as well as add static environmental variables if provided.\n\ndf &lt;- merge_spp_env(rastStack = combinedRasts, envData = norm, addStatic = TRUE, staticData = './Data/staticVariables_cropped_normZ.RData')\n#combinedRasts is the combined rasterStack for the species created by the function 'merge_rasts'\n#staticData is the path to the RData object containing a list of static environmental rasters \n\nThe resulting data frame will have the following structure:\n\n\n'data.frame':   100 obs. of  26 variables:\n $ x         : num  -66.8 -66.7 -66.7 -66.6 -66.5 ...\n $ y         : num  45 45 45 45 45 ...\n $ variable  : chr  \"X01.1993\" \"X01.1993\" \"X01.1993\" \"X01.1993\" ...\n $ value     : num  0 0 0 0 0 0 0 0 0 0 ...\n $ month     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ year      : num  1993 1993 1993 1993 1993 ...\n $ bottomT   : num  0.182 0.179 0.178 0.172 0.17 ...\n $ bottomO2  : num  -0.14 -0.141 -0.147 -0.145 -0.148 ...\n $ bottomS   : num  0.00701 0.00852 0.01051 0.01072 0.01158 ...\n $ bottomArg : num  -0.000851 -0.000806 -0.000774 -0.000747 -0.000733 ...\n $ surfaceT  : num  0.123 0.126 0.129 0.127 0.125 ...\n $ surfaceS  : num  -0.02366 -0.01261 -0.00579 -0.00424 -0.00458 ...\n $ surfacepH : num  -0.1078 -0.1075 -0.1067 -0.1008 -0.0945 ...\n $ MLD       : num  -0.0925 -0.1174 -0.1207 -0.1069 -0.1362 ...\n $ diazPP    : num  0.132 0.137 0.143 0.145 0.145 ...\n $ smallPP   : num  0.038 0.0368 0.0471 0.0567 0.0567 ...\n $ mediumPP  : num  -0.0751 -0.0712 -0.0546 -0.0295 -0.0275 ...\n $ largePP   : num  -0.026092 -0.024324 -0.01777 -0.007 -0.000244 ...\n $ smallZoo  : num  -0.0764 -0.0848 -0.1159 -0.116 -0.172 ...\n $ mediumZoo : num  -0.0322 -0.0385 -0.0549 -0.0562 -0.0859 ...\n $ largeZoo  : num  0.0372 0.0369 0.0403 0.0377 0.0361 ...\n $ intNPP    : num  -0.043317 -0.03148 -0.016401 0.000677 0.000198 ...\n $ POC       : num  0.00419 -0.00173 -0.00279 -0.001 -0.00336 ...\n $ bathy     : num  -79 -51.6 -62.6 -57.6 -69.2 ...\n $ rugosity  : num  0.512 1.224 0.597 0.132 0.236 ...\n $ dist2coast: num  4.665 0.926 3.021 9.073 10.255 ...\n\n\nThe ‘x’, ‘y’, ‘variable’, ‘value’, ‘month’, ‘year’ columns will always be present. ‘variable’ is equal to the name of the rasterStack layers, which should be in MM.YYYY format to pull month and year. ‘value’ is the value in the fisheries raster grid cell and should be equal to 0, 1, or 2.\nAdditional functions are provided to clean/simplify the data frame: match_guilds, clean_data, and remove_corr. Of these, only clean_data is required, remove_corr is recommended, and match_guilds is optional.\nThese could be run in any order, with the exception of if the user is utlizing the match_guilds function. This should be run prior to remove_corr if using.\nHere, we will run match_guilds first. This function uses the habitat and feeding guilds keys to subset the columns in the data frame to only those relevant to the target species. For example, you may have environmental covariates from the ocean surface and sea floor, like surface and bottom temperatures. Species more associated with the sea floor, such as groundfish or flatfish, are more likely to be impacted by bottom temperatures than surface temperatures they may rarely interact with.\n\ndfG &lt;- match_guilds(spp_env = df, spp = c(\"PARALICHTHYS DENTATUS\"), spp_col = 'SCI_NAME', spp_guild = 'spp_list.csv', feeding_key = 'feeding_guilds.csv', feeding_col = 'Feeding.Guild', habitat_key = 'habitat_guilds.csv',  habitat_col = 'Habitat.Guild', static_vars = c('x', 'y', 'month', 'year', 'bathy', 'rugosity', 'dist2coast'), pa_col = 'value')\n\nFor example, based on the habitat and feeding keys provided, the data frame for Summer flounder will be subset to the following variables:\n\n\n'data.frame':   1000 obs. of  15 variables:\n $ x         : num  -66.8 -66.7 -66.7 -66.6 -66.5 ...\n $ y         : num  45 45 45 45 45 ...\n $ value     : num  0 0 0 0 0 0 0 0 0 0 ...\n $ month     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ year      : num  1993 1993 1993 1993 1993 ...\n $ bottomT   : num  0.182 0.179 0.178 0.172 0.17 ...\n $ bottomO2  : num  -0.14 -0.141 -0.147 -0.145 -0.148 ...\n $ bottomS   : num  0.00701 0.00852 0.01051 0.01072 0.01158 ...\n $ bottomArg : num  -0.000851 -0.000806 -0.000774 -0.000747 -0.000733 ...\n $ smallZoo  : num  -0.0764 -0.0848 -0.1159 -0.116 -0.172 ...\n $ mediumZoo : num  -0.0322 -0.0385 -0.0549 -0.0562 -0.0859 ...\n $ largeZoo  : num  0.0372 0.0369 0.0403 0.0377 0.0361 ...\n $ bathy     : num  -79 -51.6 -62.6 -57.6 -69.2 ...\n $ rugosity  : num  0.512 1.224 0.597 0.132 0.236 ...\n $ dist2coast: num  4.665 0.926 3.021 9.073 10.255 ...\n\n\nNext we will run clean_data to turn the data into “true” binary data. The fisheries rasters, and resulting data frames, had values ranging from 0 to 2, representing no effort (0), effort but no catch (1), and catch (2). For presence/absence modeling, the only accepted values are 0 and 1, and these models do not need to know where there was an absence of effort (while is generally good for users to know to understand the scope of the fishing effort). The function clean_data will remove all existing 0 values, and remap the presence and absence data to 1s and 0s.\n\ndfB &lt;- clean_data(dfG, pa_col = 'value')\n\nThe histograms below illustrate how the data have been transformed.\n\n\n\n\n\n\n\n\n\nNote that these datasets are random subsets of the complete dataset so the number of presences/absences is not equal in each. These histograms are only to illustrate the shift in range from 0-2 to 0-1.\nFinally, we will run remove_corr to remove any correlated environmental covariates before building the model. Position and time (month & year) are retained.\n\ndfC &lt;- remove_corr(dfB, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year')\n\nIf covariates are removed due to correlation with other covariates, the corresponding column names will be printed in the console or the log file.\nThe wrapper function makeDF includes all of the functions related to making and cleaning the data frame and provides skip and log file functionalities. This function also makes sure that the combined fisheries raster is loaded and the layers have the appropriate names. Like other wrapper functions, this function also saves the results from each call in the appropriate species-specific folder.\nThis function is generally not very computationally expensive (takes &lt;10 minutes), unless you are matching a lot of environmental covariates. If you need to perform this operation for a long list of species, the wrapper function is designed to be run in parallel.\n\nload('./Data/MOM6/norm_MOM6_092025.RData') #normalized environmental data needs to be loaded prior to launching the code in parallel\noptions(future.globals.maxSize = Inf) #remove check for sharing large files so that norm is shared across workers since this is a relatively low memory intensive job otherwise\nplan(multisession, workers = 5)\ndfs &lt;- future_pmap(list(..1 = args$name, ..2 = args$skip, ..3 = args$mMin, ..4 = args$mMax, ..5 = args$yMin, ..6 = args$yMax), ~ makeDF(name = ..1, skip = ..2, mMin = ..3, mMax = ..4, yMin = ..5, yMax = ..6), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#building-predicting-species-distribution-models",
    "href": "content/ensemble-sdms.html#building-predicting-species-distribution-models",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "Building & Predicting Species Distribution Models",
    "text": "Building & Predicting Species Distribution Models\nNow that the data frames have been constructed and cleaned, you can build the species distribution models. A series of functions are provided to build, perform cross-validation on, evaluate, and extract variable importance from the models for each of the component models for the ensemble: Generalized Additive Model (GAM), MAXENT, Random Forest (RF), Boosted Regression Trees (BRT), sdmTMB.\nThe first step is building the base model:\n\n mod &lt;- make_sdm(se = dfC, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year', model = model)\n\nmodel is one of the five component models: gam, maxent, rf, brt, sdmtmb or the ensemble (ens). This function first creates the base model and simplifies it as necessary.\nThe sdm_cv function performs a cross-validation of the model with k = 5.\n\n cv &lt;- sdm_cv(mod = mod, se = dfC, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year', model = model)\n\nThe predicted values are extracted from the cross-validation output to help build the ensemble and calculate the evaluation metric using the sdm_preds function.\n\n preds &lt;- sdm_preds(cv = cv, model = model)\n\nThe model is then evaluated using the predicted values. This function can calculate a Root Mean Squared Error (RMSE) or Area under the Curve (AUC).\n\n ev &lt;- sdm_eval(preds = preds, metric = 'auc', model = model)\n\nVariable importance for each of the ensemble model components can be extracted from the models:\n\nimp &lt;- sdm_importance(mod = mod, se = dfC, pa_col = 'value', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year', model = model)\n\nIt is important to note that variable importance from the different models should not be compared without normalizing the values between 0 and 1 as they are all generated using different methods.\nThe wrapper function makeMods combines the above 5 functions, and includes skip and logging functionality. Like previous wrapper functions, it is easy to launch this in parallel with the furrr functionality.\n\nplan(multisession, workers = 5)\nchecks &lt;- future_pmap(list(..1 = args$spp, ..2 = args$model, ..3 = args$skip), ~ makeMods(spp = ..1, model = ..2, skip = ..3), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)\n\nWhere args is the combination of target species and models to be run, along with the flag to enable the skip functionality.\nHere, it is important to note that the different models have different memory demands. The GAM, RF, and MAXENT models are the least computationally expensive, followed by BRT. The sdmTMB is the most expensive. Run times will depend on the dataset size, computational resources, and your R environment. Therefore, it is recommended that you generate sdmTMB models in sequence, but all other models can be generated in parallel.\nBelow are example run times and run conditions for the final model runs for the Northeast CVA2.0 with 36 target species. This includes all of the steps outlined above: 1) building & simplying the model, 2) performing cross validations, and extracting 3) predictions, 4) AUC, and 5) variable importance. All models were run in a remote R terminal, in an environment linked to Intel’s oneAPI, which includes a BLAS library (sdmTMB recommends linking R to an optimized BLAS library to improve runtimes; see the sdmTMB installation guide), with 96 GB of available memory. Final datasets for each species had 100,000+ observations and were not downsampled. The exception to this was the random forest models which perform better with even distributions of presences and absences. All presences were retained and absences were randomly selected across regions in time and space to ensure an even distribution of absences.\n\nModel Run Times\n\n\n\n\n\n\n\n\n\n\n\nGAM\nMAXENT\nRF\nBRT\nsdmTMB\n\n\n\n\nNumber of Cores\n10\n12\n6\n10\n1\n\n\nTotal Run Time (hrs)\n15\n60\n2\n26\n82\n\n\nAverage Model Run Time (hrs)*\n4.2\n20*\n0.3\n7.2\n2.5\n\n\n\n\nNote 1: These are average run times calculated by dividing the total run time by the number of models run per core (36/number of cores). Some individual models were built faster or slower than the times listed due to the number of presences and the complexity of the environmental relationships\nNote 2: The addition of the oneAPI BLAS library increased MAXENT model run times in comparison to previous runs in environments not linked to oneAPI. These previous MAXENT run times in a standard R environment were more similar to GAM run times. However, sdmTMB run times decreased by ~2x (based on small scale tests), but needed to be run in sequence due to memory constraints (yes, even with 96GB of memory, there were memory contrains due to the size of the data set and memory needs of the model), whereas MAXENT models were memory-light enough to be run in parallel.\n\nThe function make_predictions will predict the model to a provided timeseries. It has the option to mask off the predictions based on a provided bathymetry raster (bathyR).\n\nload('./Data/MOM6/norm_MOM6_082025.RData') #list of normalized environmental rasterStacks to predict to; number of layers will dictate the time series predicted to\nabund &lt;- make_predictions(mod = mod, model = model, rasts = norm, mask = T, bathyR = bathyR, bathy_max = 1000, se = dfC, staticData = './Data/staticVariables_cropped_normZ.RData', xy_col = c('x', 'y'), month_col = 'month', year_col = 'year')\n\nThe output will be a rasterStack with the same resolution and number of layers as the time series of environmental data provided.\nThe wrapper function predictMods loads the necessary data frames and models, and saves the output to the output_rasters directory within the working directory. Similar to other wrapper functions, it also includes skip and logging functionality, and it is written to be easily run in parallel.\n\nplan(multisession, workers = 5)\nchecks &lt;- future_pmap(list(..1 = args$spp, ..2 = args$model, ..3 = args$skip), ~ predictMods(spp = ..1, model = ..2, skip = ..3), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)\n\nIt is important to note that for whatever reason, the MAXENT model cannot be predicted in parallel, so it must be run in sequence. However, predicting the models is much quicker and less memory-intensive than making the models themselves.\nmake_predictions will produce a list of rasters, one raster for each time in the time series, that can be converted to a rasterStack with the stack function. Predictions will range between 0 and 1, representing the probability of a species occuring in that grid cell at that time. If mask = T, predictions will be limited to a certain depth range. For example if bathy_max = 1000 then predictions will not be made for areas where the bathymetry is greater than 1000 m as in the example from a GAM model for Atlantic cod below: \n\nAlternative for sdmTMB Predictions\nRegardless of working environment, predictions of sdmTMB models with predictMods can take significant amounts of time. The functions makePredDF and predictSDM offer an alternative workflow similar to predictMods. While predictMods performs the predictions for each timestep, this workflow generates a large dataframe of all environmental raster data across all timesteps, performs the prediction once for all timesteps, then creates the individual rasters for each timestep.\n\n#all arguments are the same as `make_predictions`\nallData &lt;- makePredDF(norm, bathyR = bathyR, bathy_max = 1000, staticData = './Data/staticVariables_cropped_normZ.RData', mask = T)\npred &lt;- predict(mod, newdata = allData, type = 'response') #predict everything all at once in a single call \npred$my &lt;- paste(pred$month, pred$year, sep = '.')\nabund &lt;- predictSDM(mod = mod, df = pred, staticData = './Data/staticVariables_cropped_normZ.RData') #make into rasters\n\nThis workflow can be easily modified to run in a loop across multiple species. It still takes significant amounts of memory (~50GB for the NE Shelf CVA2.0) so running in parallel is not recommended. Luckily, this code takes approximately 25 minutes to run for a single species.",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/ensemble-sdms.html#building-predicting-ensemble",
    "href": "content/ensemble-sdms.html#building-predicting-ensemble",
    "title": "Building & Predicting Ensemble Species Distribution Models",
    "section": "Building & Predicting Ensemble",
    "text": "Building & Predicting Ensemble\nThe ensemble is built using the framework from the EFHSDM package. Here it is included in the make_sdm function:\n\nens &lt;- make_sdm(model = 'ens', ensembleWeights = weights, ensemblePreds = pds)\n\nWhere the weights is a vector of model weights produced either by the EFHSDM function MakeEnsemble if using RMSE or another method and pds is a list of prediction outputs from the cross-validation of the models. The weights vector and predictions list must be in the same order to correctly assign weights to predictions.\nTo predict the ensemble model, the make_predictions function is used:\n\nabund &lt;- make_predictions(model = 'ens', rasts = abds, weights = weights, staticData = NULL, mask = F, bathy_nm = NULL, bathy_max = NULL, se = NULL, month_col = NULL, year_col = NULL, xy_col = NULL)\n\nWhere abds is a list of predicted rasters from each of the component models. Similar to building the model, the order of the rasters must match the order of the models in the weights vector so that the weights can be assigned appropriately. To predict the ensemble, which is just a weighted average of the component model predictions, only a list of predictor rasters and the model weights are required, so many of the other arguments are set to NULL.\nThe wrapper function makeEns is provided to combine both the creation and prediction of the ensemble. Creating and predicting the ensemble is fast and quick since it is just performing a weighted average of the existing data, so no skip functionality is provided, but logging functionality remains. This can be run in parallel if desired.\n\nplan(multisession, workers = 5)\nchecks &lt;- future_pmap(list(..1 = spp.list$Name), ~ makeEns(spp = ..1), .progress = T,  .options = furrr_options(seed = 2025))\nplan(sequential)\n\nThe resulting list of rasters, which is again a list of raster layers, will be a weighted average of all provided model predictions. So, like the original predictions the values will range between 0 and 1.",
    "crumbs": [
      "R Package",
      "Building & Predicting Ensemble Species Distribution Models"
    ]
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File &gt; New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools &gt; Global Options &gt; R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/sdm-methods.html",
    "href": "content/sdm-methods.html",
    "title": "Species Distribution Modeling",
    "section": "",
    "text": "The first step in the CVA2.0 is the construction of species distribution models (SDMs). SDMs are statistical models that describe the relationships between species presence or abundance and environmental attributes or covariates. There are many different types of SDMs that use different data types and statistical methods. There is an abundance of scientific literature that both describe and compare different model types. We will provide a brief overview below and describe the methods selected for the CVA2.0.",
    "crumbs": [
      "Methods",
      "Species Distribution Modeling"
    ]
  },
  {
    "objectID": "content/sdm-methods.html#presenceabsence-vs-abundance-data",
    "href": "content/sdm-methods.html#presenceabsence-vs-abundance-data",
    "title": "Species Distribution Modeling",
    "section": "Presence/Absence vs Abundance Data",
    "text": "Presence/Absence vs Abundance Data\nSpecies distribution models can be built with primarily two different kinds of species data - presence/absence or abundance data. Presence/absence data describes when a species was present or absent, usually in binary with 0s representing absences and 1s representing presences. SDMs built with presence/absence data predict how likely it is that a species will be found there - often called habitat suitability - but does not consider how many individuals will be present.\nAbundance data on the other hand uses count or species density estimates. SDMS built with abundance data can predict the abundance or density of species in a given area or at a given time, but must deal with often irregular data.\n\nIn the CVA2.0…\nFor the CVA2.0, we use presence/absence data from a variety of sources. Using presence/absence data will allow us to combine multiple datasets across fisheries independent and dependent surveys.",
    "crumbs": [
      "Methods",
      "Species Distribution Modeling"
    ]
  },
  {
    "objectID": "content/sdm-methods.html#statistical-modeling",
    "href": "content/sdm-methods.html#statistical-modeling",
    "title": "Species Distribution Modeling",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\nThere are many statistical models that we could have chosen for our SDMs. Options included generalized linear models (GLMs), generalized additive models (GAMs), uni (single) or multi-variate state-space models, machine learning methods, decision tree methods, and more! Many models have been used to describe species distributions along the Northeast Shelf. Each model type has pros and cons, and different models use different assumptions, statistical methods, etc to define habitat preferences.\nTo account for some of these differences between models, model ensembles can be generated by combining different model outputs together. Ensembles are usually built by averaging the outputs of different component models. Weighted averages can also be used to weight the averages based on model performance.\n\nIn the CVA2.0…\nWe use an ensemble modeling approach for the CVA2.0, following the work of the Alaska Fisheries Science Center (AFSC)’s Groundfish program, recently implimented an ensemble modeling framework for their Essential Fish Habitat work. Here, we use the following models:\n\nGeneralized Additive Model (GAM) using methods in EFHSDM\nMAXimum ENTropy (MAXENT) using methods in EFHSDM\nRandom Forest with Spatial Interpolation (RFSI)\nBoosted Regression Trees (BRTs) using gbm and following methods from Braun et al 2023\nSpatio-temporal Generalized Linear Mixed Models (GLMMs) with sdmTMB\n\nAll of these models accept presence/absence data; can be made spatially and temporally explicit, which was important for our analysis; and combine more “traditional” SDM methods such as GAM and MAXENT included in the AFSC’s ensemble with machine learning/decision tree-based models such as RFSI and BRTs. sdmTMB models are similar to other SDM methods common in fisheries work like VAST and tinyVAST. In addition, these or similar models have been used on the Northeast Shelf in previously published works.\nWe will generate our ensemble model by performing a weighted average on our components’ outputs. Weights will be calculated using the same methods as the AFSC. However, we will use Area under the Curve (AUC) rather than root mean squared error (RMSE) because AUC is a better accuracy metric for presence/absence models than RMSE.",
    "crumbs": [
      "Methods",
      "Species Distribution Modeling"
    ]
  },
  {
    "objectID": "content/sensitivity.html",
    "href": "content/sensitivity.html",
    "title": "Sensitivity",
    "section": "",
    "text": "Species sensitivity to change will be measured qualitatively as in the original CVA. Species experts will rank the impacts of different life history attributes on species sensitivity.\n\n\nThe Boyce et al 2022 and 2024 papers seperated species sensitivity and adaptability attributes. In the original CVA, sensitivity and adaptive attributes were grouped together, as removing one or the other significantly changed the results. Furthermore, Morrison et al 2015 considered sensitivity and adaptivity inverses of each other (ie higher sensitivity would mean that a species is less adaptive) and decided to group them.\n##Sensitivity Attributes The sensitivity attributes used in the CVA2.0 were similar to those used in the Northeast US for the original. A handful were updated to reflect new insights into the impacts of climate change on species life histories since the original analysis.\nThe following sensitivity attributes were used in the CVA2.0 analysis for the Northeast US. Updated attributes are bolded.\n\nJuvenile Habitat Requirements\nPrey Specificity\nRelationship to Calcified Organisms\nComplexity in Reproductive Strategy\nHistorical Variability in Temmperature\nEgg and Larval Survival and Settlement Requirements\nStock Size/Status\nOther Stressors\nPopulation Growth Rate\nDispersal of Early Life Stages\nAdult Mobility\nSpawning Cycle\n\nFull text of the attributes will be made available once finalized.",
    "crumbs": [
      "Methods",
      "Sensitivity"
    ]
  },
  {
    "objectID": "content/sensitivity.html#overview",
    "href": "content/sensitivity.html#overview",
    "title": "Sensitivity",
    "section": "",
    "text": "Species sensitivity to change will be measured qualitatively as in the original CVA. Species experts will rank the impacts of different life history attributes on species sensitivity.\n\n\nThe Boyce et al 2022 and 2024 papers seperated species sensitivity and adaptability attributes. In the original CVA, sensitivity and adaptive attributes were grouped together, as removing one or the other significantly changed the results. Furthermore, Morrison et al 2015 considered sensitivity and adaptivity inverses of each other (ie higher sensitivity would mean that a species is less adaptive) and decided to group them.\n##Sensitivity Attributes The sensitivity attributes used in the CVA2.0 were similar to those used in the Northeast US for the original. A handful were updated to reflect new insights into the impacts of climate change on species life histories since the original analysis.\nThe following sensitivity attributes were used in the CVA2.0 analysis for the Northeast US. Updated attributes are bolded.\n\nJuvenile Habitat Requirements\nPrey Specificity\nRelationship to Calcified Organisms\nComplexity in Reproductive Strategy\nHistorical Variability in Temmperature\nEgg and Larval Survival and Settlement Requirements\nStock Size/Status\nOther Stressors\nPopulation Growth Rate\nDispersal of Early Life Stages\nAdult Mobility\nSpawning Cycle\n\nFull text of the attributes will be made available once finalized.",
    "crumbs": [
      "Methods",
      "Sensitivity"
    ]
  },
  {
    "objectID": "content/sensitivity.html#expert-scoring",
    "href": "content/sensitivity.html#expert-scoring",
    "title": "Sensitivity",
    "section": "Expert Scoring",
    "text": "Expert Scoring\nExperts across NOAA, research, and industry were asked to score how species life history traits may make them sensitive to climate change, following the original methods in Morrison et al 2015. Experts were provided basic life history information relevant to the sensitivity attributes. The species profiles from the original analysis were updated and reviewed for consistency by 2 NOAA scientists. Experts were asked to score species both within and outside their field of expertise. Each species was scored by five experts, with at least two scorers per species considered experts on that species guild.\nBriefly, experts assign a score based on four scoring bins (low, moderate, high, and very high) for each attribute using their expert judgement and life history information. Each expert had five tallies to distribute across the four bins to help quantify certainty. If an expert was certain about a score, they could put all five tallies in a single bin. Conversely, if they were uncertain, the tallies could be distributed across multiple bins. Certainty was quantified using a bootstrap analysis by resampling tallies with replacement and recalculating the attribute and sensitivity scores.\nExperts were asked to score species independently, and then met for three virtual discussion workshops to discuss scores with other experts. They were then given the chance to finalize their scores based on these discussions. We asked experts to not consider the scores from the original analysis to reduce bias in the results.",
    "crumbs": [
      "Methods",
      "Sensitivity"
    ]
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/rmarkdown.html",
    "href": "content/rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "You can include R Markdown files in your project."
  },
  {
    "objectID": "content/rmarkdown.html#r-markdown",
    "href": "content/rmarkdown.html#r-markdown",
    "title": "R Markdown",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/rmarkdown.html#including-plots",
    "href": "content/rmarkdown.html#including-plots",
    "title": "R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  }
]