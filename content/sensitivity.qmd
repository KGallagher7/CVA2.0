---
title: Sensitivity
---

## Overview
Species sensitivity to change will be measured qualitatively as in the original CVA. Species experts will rank the impacts of different life history attributes on species sensitivity.  

### A Note on Sensitivity vs Adaptability 
The Boyce et al 2022 and 2024 papers seperated species sensitivity and adaptability attributes. In the original CVA, sensitivity and adaptive attributes were grouped together, as removing one or the other significantly changed the results. Furthermore, [Morrison et al 2015](http://www.st.nmfs.noaa.gov/Assets/ecosystems/climate/documents/TM%20OSF3.pdf) considered sensitivity and adaptivity inverses of each other (ie higher sensitivity would mean that a species is less adaptive) and decided to group them. 

##Sensitivity Attributes
The sensitivity attributes used in the CVA2.0 were similar to those used in the Northeast US for the original. A handful were updated to reflect new insights into the impacts of climate change on species life histories since the original analysis. 

The following sensitivity attributes were used in the CVA2.0 analysis for the Northeast US. Updated attributes are bolded.  

* **Juvenile Habitat Requirements**
* Prey Specificity
* **Relationship to Calcified Organisms**
* **Complexity in Reproductive Strategy**
* **Historical Variability in Temmperature**
* **Egg and Larval Survival and Settlement Requirements**
* Stock Size/Status
* **Other Stressors**
* Population Growth Rate
* Dispersal of Early Life Stages
* Adult Mobility
* Spawning Cycle

Full text of the attributes will be made available once finalized.

## Expert Scoring
Experts across NOAA, research, and industry were asked to score how species life history traits may make them sensitive to climate change, following the original methods in [Morrison et al 2015](http://www.st.nmfs.noaa.gov/Assets/ecosystems/climate/documents/TM%20OSF3.pdf). Experts were provided basic life history information relevant to the sensitivity attributes. The species profiles from the original analysis were updated and reviewed for consistency by 2 NOAA scientists. Experts were asked to score species both within and outside their field of expertise. Each species was scored by five experts, with at least two scorers per species considered experts on that species guild. 

Briefly, experts assign a score based on four scoring bins (low, moderate, high, and very high) for each attribute using their expert judgement and life history information. Each expert had five tallies to distribute across the four bins to help quantify certainty. If an expert was certain about a score, they could put all five tallies in a single bin. Conversely, if they were uncertain, the tallies could be distributed across multiple bins. Certainty was quantified using a bootstrap analysis by resampling tallies with replacement and recalculating the attribute and sensitivity scores. 

Experts were asked to score species independently, and then met for three virtual discussion workshops to discuss scores with other experts. They were then given the chance to finalize their scores based on these discussions. We asked experts  to **not** consider the scores from the original analysis to reduce bias in the results. 

## Analysis
Scores were collected using the Fish Stock Climate Vulnerability Assessment Portal, and exported as a csv. Scores were calculated for each attribute and these scores were used to calculate a final exposure score for the species. Unlike exposure, and similar to the original analysis, a single value is produced for each species.  

### Attribute Scores
Attribute scores were calculated using a weighted average following Equation 1 in Morrison et al 2015. Briefly, the number of tallies in each of the four bins is multiplied by a weight, summed, and divided by the total number of tallies (which would be 5 scorers per species x 5 tallies per scorer = 25). The weights that correspond to each bin are: 

|Bin| Weight | 
|:---------:|:---------:|
| Low | 1   | 
| Medium | 2  | 
| High |  3 |  
| Very High | 4 |
: Bin Weights

The function `attribute.score` performs the weighted average. This function is based on code provided by the Tyler Loughran and the Atlantic Highly Migratory Species Management Division, Office of Sustainable Fisheries. The workflow is outlined below. 

### Total Sensitivity Scores
Following the original CVA methods, total sensitivity is calculated with a logic rule. This is the same logic rule used to calculate total exposure from individual variable exposure scores: 

|Overall Score| Numeric Score | Logic Rule |
|:---------:|:---------:|:---------:|
| Low | 1   |   All other scores  |
| Medium | 2  | 2 or more attributes >= 2.5   |  
| High | 3 |  2 or more attributes >= 3.0  |
| Very High | 4 | 3 or more attributes >= 3.5 |
: Logic Rule used to Calculate Total Species Sensitivity

The total sensitivity score is calculated using the `logic.rule` function.

### Workflow
#### Calculating Scores
A single function, `calculate.sensitivity`, combines both the attribute score calculation done by `attribute.score` and the total sensitivity score calculation done by `logic.rule` in a single function. This function is modeled on provided code from the Southeast Fisheries Science Center and built to use `lapply` functionality to move through all species quickly. 

```{r}
#| eval: false
data <-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal
species.data.list <- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list
species.sensitivity <- lapply(species.data.list, calculate.sensitivity, bootstrap = F) #calculate attribute scores and total sensitivity 
```
The resulting data.frame contains a row for each species scored, with columns containing the attribute scores and a total sensitivity score. 

#### Calculating Uncertainty
Uncertainty was quantified using the tally-based scoring system described above. Uncertainty was calculated by resampling the tallies with replacement across the four bins, and recalculating the weighted average attribute score, then the total sensitivity. Following the original CVA, uncertainty is represented as the proportion or percent of bootstrapped total sensitivity scores that match the expert-derived total scores. For the default number of iterations (10,000), this function takes just under a minute (~45 seconds) per species on a standard workstation. 

Both 'attribute.score' and 'logic.rule', and subsequently 'calculate.sensitivity' have the ability to also perform the bootstrap calculation. 
```{r}
#| eval: false
data <-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal
species.data.list <- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list
sensitivity.bootstrap <- lapply(species.data.list, calculate.sensitivity, bootstrap = T, samples = 10000) #calculate sensitivity with bootstrapping 
```
When `bootstrap = TRUE`, the resulting data.frame will have a number of rows equal to the number of `samples`, with each row representing one of the iterations. Columns will contain the attribute scores and total sensitivity score for that iteration. 

The function `bootstrap.certainty` calculates the final uncertainty value (AKA the proportion of bootstrap iterations that match the expert scores). This function will calculate the proportion of bootstrapped total sensitivity scores correspond to each possible score (1-4). If `match = TRUE`, then the corresponding proportion to the score derived from the expert tallies will be appended to the output from `calculate.sensitivity(bootstrap = TRUE)` as a column called `Certainty`. If `match = FALSE`, a vector containing the proportion of bootstrap scores that correspond to each possible total sensitivity score is returned.  

As such, the total workflow can be run with only 5 lines of code: 
```{r}
#| eval: false
data <-read.csv('expert_scores.csv') #raw sensitivity data from FSCVA Portal
species.data.list <- split(data, data$Stock.Name) #generate a list of data.frames, where each species has its own dedicated data frame of scores from all scorers in the list
species.sensitivity <- lapply(species.data.list, calculate.sensitivity, bootstrap = F) #calculate attribute scores and total sensitivity
sensitivity.bootstrap <- lapply(species.data.list, calculate.sensitivity, bootstrap = T, samples = 10000) #calculate sensitivity with bootstrapping 
sensitivity.certainty <- lapply(sensitivity.bootstrap, bootstrap.certainty, match = T, sensitivity.dataframe = species.sensitivity)
```
